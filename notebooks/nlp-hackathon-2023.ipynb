{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "data = []\n",
    "with io.open(\"/root/data/test.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    word = []\n",
    "    token = []\n",
    "    for line in f:\n",
    "        fields = line.strip().split(\" \")\n",
    "        if fields[0] == \"\":\n",
    "            if len(word)>0:\n",
    "                data.append((word, token))\n",
    "            word = []\n",
    "            token = []\n",
    "            continue\n",
    "        word.append(fields[0].strip())\n",
    "        token.append('O')\n",
    "    data.append((word, token))\n",
    "    \n",
    "\n",
    "#convert to pandas dataframe\n",
    "import pandas as pd\n",
    "df_test = pd.DataFrame(data, columns=['tokens', 'tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[নিকটতম, বিমানবন্দরটি, (, ১৫৮, কিমি, ), রাজীব,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[এটি, ব্যাখ্যা, করে, যে, কেন, একটি, অফ, ব্র্যা...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[ওয়ালমার্ট, মাফিয়া, ২]</td>\n",
       "      <td>[O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[স্টিলের, দোকান, বানোয়াট, (, নিকটবর্তী, ট্যাম...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[বুগাটি, চিরন, কে, কিভাবে, ইন্টারনেটে, সংযুক্ত...</td>\n",
       "      <td>[O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13210</th>\n",
       "      <td>[তিনি, বসেরা, এক, হি, ভুল, জীবন, ধারা, এবং, অগ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13211</th>\n",
       "      <td>[তিনি, রিচার্ড, স্কট, এর, দ্বিতীয়ার্ধের, বিকল...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13212</th>\n",
       "      <td>[সিগেট, টেকনোলজি, এর, সদর, দপ্তর, কোথায়]</td>\n",
       "      <td>[O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13213</th>\n",
       "      <td>[এই, শিখরের, হিমবাহ, স্ট্যাটাস, এবং, এর, পরিসী...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13214</th>\n",
       "      <td>[প্রতিক্রিয়া, হিসাবে, ডাব্লিউডাব্লিউই, বিপদগু...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13215 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tokens  \\\n",
       "0      [নিকটতম, বিমানবন্দরটি, (, ১৫৮, কিমি, ), রাজীব,...   \n",
       "1      [এটি, ব্যাখ্যা, করে, যে, কেন, একটি, অফ, ব্র্যা...   \n",
       "2                               [ওয়ালমার্ট, মাফিয়া, ২]   \n",
       "3      [স্টিলের, দোকান, বানোয়াট, (, নিকটবর্তী, ট্যাম...   \n",
       "4      [বুগাটি, চিরন, কে, কিভাবে, ইন্টারনেটে, সংযুক্ত...   \n",
       "...                                                  ...   \n",
       "13210  [তিনি, বসেরা, এক, হি, ভুল, জীবন, ধারা, এবং, অগ...   \n",
       "13211  [তিনি, রিচার্ড, স্কট, এর, দ্বিতীয়ার্ধের, বিকল...   \n",
       "13212          [সিগেট, টেকনোলজি, এর, সদর, দপ্তর, কোথায়]   \n",
       "13213  [এই, শিখরের, হিমবাহ, স্ট্যাটাস, এবং, এর, পরিসী...   \n",
       "13214  [প্রতিক্রিয়া, হিসাবে, ডাব্লিউডাব্লিউই, বিপদগু...   \n",
       "\n",
       "                                                    tags  \n",
       "0                      [O, O, O, O, O, O, O, O, O, O, O]  \n",
       "1      [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "2                                              [O, O, O]  \n",
       "3      [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "4                                  [O, O, O, O, O, O, O]  \n",
       "...                                                  ...  \n",
       "13210  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "13211                           [O, O, O, O, O, O, O, O]  \n",
       "13212                                 [O, O, O, O, O, O]  \n",
       "13213   [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]  \n",
       "13214  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "\n",
       "[13215 rows x 2 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('/root/data/test.jsonl', 'w', encoding='utf8') as f:\n",
    "    for index, row in df_test.iterrows():\n",
    "        tokens = row['tokens']\n",
    "        f.write(json.dumps({'tokens': tokens, 'tags': row['tags']}, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyring is skipped due to an exception: 'keyring.backends'\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (1.13.1)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (0.14.1)\n",
      "Requirement already satisfied: torchaudio in /opt/conda/lib/python3.7/site-packages (0.13.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/lib/python3.7/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/lib/python3.7/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/lib/python3.7/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/lib/python3.7/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.34.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (59.3.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision) (1.21.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision) (9.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision) (1.26.13)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision) (2.8)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision) (2.0.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "data = []\n",
    "with io.open(\"/root/data/train.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    word = []\n",
    "    token = []\n",
    "    for line in f:\n",
    "        fields = line.strip().split(\" \")\n",
    "        if len(fields) == 1:\n",
    "            data.append((word, token))\n",
    "            word = []\n",
    "            token = []\n",
    "            continue\n",
    "        word.append(fields[0].strip())\n",
    "        token.append(fields[3].strip())\n",
    "    data.append((word, token))\n",
    "\n",
    "#convert to pandas dataframe\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(data, columns=['tokens', 'tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[তার, মৃত্যুর, দশ, দিন, পর,, ১১৫, কৃষ্ণাঙ্গ, উ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, B-LOC,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[ব্রাংম্যান, ডাম্পসন, ১৪০০, সালে, আন্তর্জাতিক,...</td>\n",
       "      <td>[O, O, O, O, B-GRP, I-GRP, I-GRP, I-GRP, I-GRP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[রাজকীয়, বাসস্থান, থেকে, রাজ্যের, মন্দির, পর্...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[তিনি, তৃতীয়, সহস্রাব্দে, গ্র্যান্ড, ওলে, অপ্...</td>\n",
       "      <td>[O, O, O, B-CW, I-CW, I-CW, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[প্রায়, ১৮,০০০, এর, উচ্চতর, অনুপাত, দোআঁশ, মা...</td>\n",
       "      <td>[O, O, O, O, O, B-PROD, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15295</th>\n",
       "      <td>[অ্যালবামটির, শিল্পকর্ম, হিপনোসিস, তৈরি, করেছে।]</td>\n",
       "      <td>[O, O, B-CORP, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15296</th>\n",
       "      <td>[ব্র্যান্ড,, তার, পিতামাতার, মত,, যারা, প্রিপে...</td>\n",
       "      <td>[O, O, O, O, O, B-PROD, I-PROD, I-PROD, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15297</th>\n",
       "      <td>[লিভারপুলের, ম্যানেজার, ইয়ুর্গেন, ক্লপ, পরে, ...</td>\n",
       "      <td>[O, O, B-PER, I-PER, O, O, O, O, O, O, O, O, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15298</th>\n",
       "      <td>[সাধারণত, নির্মাণে, একটি, নতুন, প্রযুক্তি, ভবন...</td>\n",
       "      <td>[O, O, O, O, O, B-PROD, I-PROD, O, O, O, O, O,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15299</th>\n",
       "      <td>[এটি, প্রথম, দরগৌদ, দ্বারা, প্রকাশিত, হয়েছিল।]</td>\n",
       "      <td>[O, O, B-CORP, O, O, O]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15300 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tokens  \\\n",
       "0      [তার, মৃত্যুর, দশ, দিন, পর,, ১১৫, কৃষ্ণাঙ্গ, উ...   \n",
       "1      [ব্রাংম্যান, ডাম্পসন, ১৪০০, সালে, আন্তর্জাতিক,...   \n",
       "2      [রাজকীয়, বাসস্থান, থেকে, রাজ্যের, মন্দির, পর্...   \n",
       "3      [তিনি, তৃতীয়, সহস্রাব্দে, গ্র্যান্ড, ওলে, অপ্...   \n",
       "4      [প্রায়, ১৮,০০০, এর, উচ্চতর, অনুপাত, দোআঁশ, মা...   \n",
       "...                                                  ...   \n",
       "15295   [অ্যালবামটির, শিল্পকর্ম, হিপনোসিস, তৈরি, করেছে।]   \n",
       "15296  [ব্র্যান্ড,, তার, পিতামাতার, মত,, যারা, প্রিপে...   \n",
       "15297  [লিভারপুলের, ম্যানেজার, ইয়ুর্গেন, ক্লপ, পরে, ...   \n",
       "15298  [সাধারণত, নির্মাণে, একটি, নতুন, প্রযুক্তি, ভবন...   \n",
       "15299    [এটি, প্রথম, দরগৌদ, দ্বারা, প্রকাশিত, হয়েছিল।]   \n",
       "\n",
       "                                                    tags  \n",
       "0      [O, O, O, O, O, O, O, O, O, O, O, O, O, B-LOC,...  \n",
       "1      [O, O, O, O, B-GRP, I-GRP, I-GRP, I-GRP, I-GRP...  \n",
       "2      [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "3          [O, O, O, B-CW, I-CW, I-CW, O, O, O, O, O, O]  \n",
       "4                    [O, O, O, O, O, B-PROD, O, O, O, O]  \n",
       "...                                                  ...  \n",
       "15295                               [O, O, B-CORP, O, O]  \n",
       "15296  [O, O, O, O, O, B-PROD, I-PROD, I-PROD, O, O, ...  \n",
       "15297  [O, O, B-PER, I-PER, O, O, O, O, O, O, O, O, O...  \n",
       "15298  [O, O, O, O, O, B-PROD, I-PROD, O, O, O, O, O,...  \n",
       "15299                            [O, O, B-CORP, O, O, O]  \n",
       "\n",
       "[15300 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "data = []\n",
    "with io.open(\"/root/data/dev.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    word = []\n",
    "    token = []\n",
    "    for line in f:\n",
    "        fields = line.strip().split(\" \")\n",
    "        if len(fields) == 1:\n",
    "            data.append((word, token))\n",
    "            word = []\n",
    "            token = []\n",
    "            continue\n",
    "        word.append(fields[0].strip())\n",
    "        token.append(fields[3].strip())\n",
    "    data.append((word, token))\n",
    "\n",
    "#convert to pandas dataframe\n",
    "import pandas as pd\n",
    "df_dev = pd.DataFrame(data, columns=['tokens', 'tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[তিনি, যুবক, হিসেবে, শেফিল্ড, বুধবার, এফ.সি., ...</td>\n",
       "      <td>[O, O, O, B-GRP, I-GRP, I-GRP, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[ভিনেগার, মাঝে, মাঝে, চাটনি, ব্যবহার, করা, হয়।]</td>\n",
       "      <td>[O, O, O, B-PROD, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[মূলত,, দুর্গটি, ছিল, একটি, সুরক্ষিত, দুর্গ,, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, B-LOC, I-LOC, I-LOC, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[হার্নান্দেজ, আলোকচিত্রগ্রাহী, ফিল্ম, ক্যামেরা...</td>\n",
       "      <td>[O, B-PROD, I-PROD, O, O, O, O, O, O, O, O, O,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[ভ্যারাইটি, সিরিজের, উজ্জ্বল,, অনলস, চেহারা, এ...</td>\n",
       "      <td>[B-CORP, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>[১৯২৯, থেকে, ১৯, সাল, পর্যন্ত, তিনি, নরস্ক, হা...</td>\n",
       "      <td>[O, O, O, O, O, O, B-CORP, I-CORP, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>[সাদারল্যান্ড, ১৮৯৯, সালে, লিচেস্টারশায়ার, কা...</td>\n",
       "      <td>[O, O, O, B-GRP, I-GRP, I-GRP, I-GRP, O, O, O,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>[জীবটি, গুহা, থেকে, পালিয়ে, গিয়ে, জেমস, নদী,...</td>\n",
       "      <td>[O, O, O, O, O, B-LOC, I-LOC, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>[টুর্নামেন্টে, বেকারের, একমাত্র, উপস্থিতি, ছিল...</td>\n",
       "      <td>[O, O, O, O, O, O, O, B-GRP, I-GRP, I-GRP, I-G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>[পুরোনো, পিসিসি, স্ট্রিটকার, শুধুমাত্র, ব্যতিক...</td>\n",
       "      <td>[O, B-PROD, I-PROD, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tokens  \\\n",
       "0    [তিনি, যুবক, হিসেবে, শেফিল্ড, বুধবার, এফ.সি., ...   \n",
       "1     [ভিনেগার, মাঝে, মাঝে, চাটনি, ব্যবহার, করা, হয়।]   \n",
       "2    [মূলত,, দুর্গটি, ছিল, একটি, সুরক্ষিত, দুর্গ,, ...   \n",
       "3    [হার্নান্দেজ, আলোকচিত্রগ্রাহী, ফিল্ম, ক্যামেরা...   \n",
       "4    [ভ্যারাইটি, সিরিজের, উজ্জ্বল,, অনলস, চেহারা, এ...   \n",
       "..                                                 ...   \n",
       "795  [১৯২৯, থেকে, ১৯, সাল, পর্যন্ত, তিনি, নরস্ক, হা...   \n",
       "796  [সাদারল্যান্ড, ১৮৯৯, সালে, লিচেস্টারশায়ার, কা...   \n",
       "797  [জীবটি, গুহা, থেকে, পালিয়ে, গিয়ে, জেমস, নদী,...   \n",
       "798  [টুর্নামেন্টে, বেকারের, একমাত্র, উপস্থিতি, ছিল...   \n",
       "799  [পুরোনো, পিসিসি, স্ট্রিটকার, শুধুমাত্র, ব্যতিক...   \n",
       "\n",
       "                                                  tags  \n",
       "0    [O, O, O, B-GRP, I-GRP, I-GRP, O, O, O, O, O, ...  \n",
       "1                           [O, O, O, B-PROD, O, O, O]  \n",
       "2    [O, O, O, O, O, O, O, B-LOC, I-LOC, I-LOC, O, ...  \n",
       "3    [O, B-PROD, I-PROD, O, O, O, O, O, O, O, O, O,...  \n",
       "4         [B-CORP, O, O, O, O, O, O, O, O, O, O, O, O]  \n",
       "..                                                 ...  \n",
       "795  [O, O, O, O, O, O, B-CORP, I-CORP, O, O, O, O, O]  \n",
       "796  [O, O, O, B-GRP, I-GRP, I-GRP, I-GRP, O, O, O,...  \n",
       "797          [O, O, O, O, O, B-LOC, I-LOC, O, O, O, O]  \n",
       "798  [O, O, O, O, O, O, O, B-GRP, I-GRP, I-GRP, I-G...  \n",
       "799                 [O, B-PROD, I-PROD, O, O, O, O, O]  \n",
       "\n",
       "[800 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                tokens  \\\n",
      "0    [তার, মৃত্যুর, দশ, দিন, পর,, ১১৫, কৃষ্ণাঙ্গ, উ...   \n",
      "1    [ব্রাংম্যান, ডাম্পসন, ১৪০০, সালে, আন্তর্জাতিক,...   \n",
      "2    [রাজকীয়, বাসস্থান, থেকে, রাজ্যের, মন্দির, পর্...   \n",
      "3    [তিনি, তৃতীয়, সহস্রাব্দে, গ্র্যান্ড, ওলে, অপ্...   \n",
      "4    [প্রায়, ১৮,০০০, এর, উচ্চতর, অনুপাত, দোআঁশ, মা...   \n",
      "..                                                 ...   \n",
      "795  [১৯২৯, থেকে, ১৯, সাল, পর্যন্ত, তিনি, নরস্ক, হা...   \n",
      "796  [সাদারল্যান্ড, ১৮৯৯, সালে, লিচেস্টারশায়ার, কা...   \n",
      "797  [জীবটি, গুহা, থেকে, পালিয়ে, গিয়ে, জেমস, নদী,...   \n",
      "798  [টুর্নামেন্টে, বেকারের, একমাত্র, উপস্থিতি, ছিল...   \n",
      "799  [পুরোনো, পিসিসি, স্ট্রিটকার, শুধুমাত্র, ব্যতিক...   \n",
      "\n",
      "                                                  tags  \n",
      "0    [O, O, O, O, O, O, O, O, O, O, O, O, O, B-LOC,...  \n",
      "1    [O, O, O, O, B-GRP, I-GRP, I-GRP, I-GRP, I-GRP...  \n",
      "2    [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
      "3        [O, O, O, B-CW, I-CW, I-CW, O, O, O, O, O, O]  \n",
      "4                  [O, O, O, O, O, B-PROD, O, O, O, O]  \n",
      "..                                                 ...  \n",
      "795  [O, O, O, O, O, O, B-CORP, I-CORP, O, O, O, O, O]  \n",
      "796  [O, O, O, B-GRP, I-GRP, I-GRP, I-GRP, O, O, O,...  \n",
      "797          [O, O, O, O, O, B-LOC, I-LOC, O, O, O, O]  \n",
      "798  [O, O, O, O, O, O, O, B-GRP, I-GRP, I-GRP, I-G...  \n",
      "799                 [O, B-PROD, I-PROD, O, O, O, O, O]  \n",
      "\n",
      "[16100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df = df.append(df_dev)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('/root/data/train.jsonl', 'w', encoding='utf8') as f:\n",
    "    for index, row in df.iterrows():\n",
    "        tokens = row['tokens']\n",
    "        f.write(json.dumps({'tokens': tokens, 'tags': row['tags']}, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('/root/data/validation.jsonl', 'w', encoding='utf8') as f:\n",
    "    for index, row in df_dev.iterrows():\n",
    "        tokens = row['tokens']\n",
    "        f.write(json.dumps({'tokens': tokens, 'tags': row['tags']}, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('/root/data/test.jsonl', 'w', encoding='utf8') as f:\n",
    "    for index, row in df_dev.iterrows():\n",
    "        tokens = row['tokens']\n",
    "        f.write(json.dumps({'tokens': tokens, 'tags': row['tags']}, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'banglabert' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/csebuetnlp/banglabert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/banglabert\n"
     ]
    }
   ],
   "source": [
    "%cd banglabert/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: / \n",
      "Found conflicts! Looking for incompatible packages.\n",
      "This can take several minutes.  Press CTRL-C to abort.\n",
      "                                                                               failed\n",
      "\n",
      "UnsatisfiableError: The following specifications were found to be incompatible with each other:\n",
      "\n",
      "Output in format: Requested package -> Available versions\n",
      "\n",
      "Package _openmp_mutex conflicts for:\n",
      "pytorch==1.8.1 -> _openmp_mutex\n",
      "pytorch==1.8.1 -> libgcc-ng[version='>=7.5.0'] -> _openmp_mutex[version='>=4.5']\n",
      "cudatoolkit=10.2 -> libgcc-ng[version='>=7.3.0'] -> _openmp_mutex[version='>=4.5']\n",
      "torchvision==0.9.1 -> pytorch==1.8.1 -> _openmp_mutex\n",
      "python==3.7.9 -> libgcc-ng[version='>=7.3.0'] -> _openmp_mutex[version='>=4.5']\n",
      "\n",
      "Package python conflicts for:\n",
      "torchaudio==0.8.0 -> numpy[version='>=1.11'] -> python[version='>=2.7,<2.8.0a0|>=3.10,<3.11.0a0|>=3.5,<3.6.0a0']\n",
      "torchvision==0.9.1 -> numpy[version='>=1.11'] -> python[version='>=2.7,<2.8.0a0|>=3.10,<3.11.0a0|>=3.5,<3.6.0a0']\n",
      "pytorch==1.8.1 -> python[version='>=3.6,<3.7.0a0|>=3.7,<3.8.0a0|>=3.8,<3.9.0a0|>=3.9,<3.10.0a0']\n",
      "torchvision==0.9.1 -> python[version='>=3.6,<3.7.0a0|>=3.7,<3.8.0a0|>=3.9,<3.10.0a0|>=3.8,<3.9.0a0']\n",
      "python==3.7.9\n",
      "pytorch==1.8.1 -> ninja -> python[version='>=2.7,<2.8.0a0|>=3.10,<3.11.0a0|>=3.5,<3.6.0a0|>=3.6|>=3.5|>=3.7|>=3.6,<3.7']\n",
      "torchaudio==0.8.0 -> python[version='>=3.6,<3.7.0a0|>=3.9,<3.10.0a0|>=3.8,<3.9.0a0|>=3.7,<3.8.0a0']\n",
      "\n",
      "Package cudatoolkit conflicts for:\n",
      "cudatoolkit=10.2\n",
      "torchaudio==0.8.0 -> pytorch==1.8.0 -> cudatoolkit[version='>=10.1,<10.2|>=11.1,<11.2|>=10.2,<10.3']\n",
      "torchvision==0.9.1 -> cudatoolkit[version='>=10.1,<10.2|>=11.1,<11.2|>=10.2,<10.3']\n",
      "pytorch==1.8.1 -> cudatoolkit[version='>=10.1,<10.2|>=11.1,<11.2|>=10.2,<10.3']\n",
      "\n",
      "Package _libgcc_mutex conflicts for:\n",
      "python==3.7.9 -> libgcc-ng[version='>=7.3.0'] -> _libgcc_mutex[version='*|0.1',build=main]\n",
      "pytorch==1.8.1 -> _openmp_mutex -> _libgcc_mutex[version='*|0.1',build=main]\n",
      "cudatoolkit=10.2 -> libgcc-ng[version='>=7.3.0'] -> _libgcc_mutex[version='*|0.1',build=main]\n",
      "\n",
      "Package typing-extensions conflicts for:\n",
      "pytorch==1.8.1 -> typing-extensions\n",
      "torchvision==0.9.1 -> pytorch==1.8.1 -> typing-extensions\n",
      "\n",
      "Package libgcc-ng conflicts for:\n",
      "python==3.7.9 -> ncurses[version='>=6.2,<7.0a0'] -> libgcc-ng[version='>=11.2.0|>=7.5.0|>=7.2.0']\n",
      "python==3.7.9 -> libgcc-ng[version='>=7.3.0']\n",
      "\n",
      "Package python_abi conflicts for:\n",
      "torchaudio==0.8.0 -> python_abi=3.9[build=*_cp39]\n",
      "torchvision==0.9.1 -> python_abi=3.9[build=*_cp39]\n",
      "\n",
      "Package libstdcxx-ng conflicts for:\n",
      "torchaudio==0.8.0 -> numpy[version='>=1.11'] -> libstdcxx-ng[version='>=11.2.0|>=7.5.0|>=7.3.0|>=7.2.0']\n",
      "pytorch==1.8.1 -> mkl[version='>=2018'] -> libstdcxx-ng[version='>=11.2.0|>=7.3.0|>=7.2.0']\n",
      "cudatoolkit=10.2 -> libstdcxx-ng[version='>=7.3.0']\n",
      "torchvision==0.9.1 -> ffmpeg[version='>=4.2'] -> libstdcxx-ng[version='>=11.2.0|>=7.3.0|>=7.5.0|>=8.4.0|>=7.2.0']\n",
      "python==3.7.9 -> libffi[version='>=3.3,<3.4.0a0'] -> libstdcxx-ng[version='>=7.3.0']\n",
      "pytorch==1.8.1 -> libstdcxx-ng[version='>=7.5.0']\n",
      "\n",
      "Package pytorch conflicts for:\n",
      "pytorch==1.8.1\n",
      "torchaudio==0.8.0 -> pytorch==1.8.0\n",
      "torchvision==0.9.1 -> pytorch==1.8.1The following specifications were found to be incompatible with your system:\n",
      "\n",
      "  - feature:/linux-64::__glibc==2.28=0\n",
      "  - feature:|@/linux-64::__glibc==2.28=0\n",
      "  - cudatoolkit=10.2 -> libgcc-ng[version='>=7.3.0'] -> __glibc[version='>=2.17']\n",
      "  - pytorch==1.8.1 -> libgcc-ng[version='>=7.5.0'] -> __glibc[version='>=2.17']\n",
      "\n",
      "Your installed version is: 2.28\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda create python==3.7.9 pytorch==1.8.1 torchvision==0.9.1 torchaudio==0.8.0 cudatoolkit=10.2 -c pytorch -p ./env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CommandNotFoundError: Your shell has not been properly configured to use 'conda activate'.\n",
      "To initialize your shell, run\n",
      "\n",
      "    $ conda init <SHELL_NAME>\n",
      "\n",
      "Currently supported shells are:\n",
      "  - bash\n",
      "  - fish\n",
      "  - tcsh\n",
      "  - xonsh\n",
      "  - zsh\n",
      "  - powershell\n",
      "\n",
      "See 'conda init --help' for more information and options.\n",
      "\n",
      "IMPORTANT: You may need to close and restart your shell after running 'conda init'.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda activate ./env # or source activate ./env (for older versions of anaconda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'transformers' already exists and is not an empty directory.\n",
      "HEAD is now at 7a26307e3 Fixes for the documentation (#13361)\n",
      "Keyring is skipped due to an exception: 'keyring.backends'\n",
      "Processing /root/banglabert/transformers\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.11.0.dev0) (2022.10.31)\n",
      "Requirement already satisfied: huggingface-hub>=0.0.12 in /opt/conda/lib/python3.7/site-packages (from transformers==4.11.0.dev0) (0.0.19)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==4.11.0.dev0) (3.0.12)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers==4.11.0.dev0) (2.28.1)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers==4.11.0.dev0) (0.10.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==4.11.0.dev0) (4.42.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers==4.11.0.dev0) (23.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers==4.11.0.dev0) (5.1.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.11.0.dev0) (1.21.6)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers==4.11.0.dev0) (0.0.53)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers==4.11.0.dev0) (6.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.0.12->transformers==4.11.0.dev0) (4.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.11.0.dev0) (3.11.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.11.0.dev0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.11.0.dev0) (1.26.13)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.11.0.dev0) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.11.0.dev0) (2022.9.24)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.11.0.dev0) (0.14.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.11.0.dev0) (1.14.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.11.0.dev0) (7.0)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-4.11.0.dev0-py3-none-any.whl size=2811112 sha256=1b46c5bb2743b05c07436e18c2ba71ada38ea4665a69a785c1bde4ec6c4e2c7a\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-6ir8js48/wheels/00/01/11/5486d97b77686b026377bf6784386bd56472c0b630831c0ffa\n",
      "Successfully built transformers\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.11.0.dev0\n",
      "    Uninstalling transformers-4.11.0.dev0:\n",
      "      Successfully uninstalled transformers-4.11.0.dev0\n",
      "Successfully installed transformers-4.11.0.dev0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mKeyring is skipped due to an exception: 'keyring.backends'\n",
      "Collecting git+https://github.com/csebuetnlp/normalizer (from -r requirements.txt (line 5))\n",
      "  Cloning https://github.com/csebuetnlp/normalizer to /tmp/pip-req-build-f__bf4te\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/csebuetnlp/normalizer /tmp/pip-req-build-f__bf4te\n",
      "  Resolved https://github.com/csebuetnlp/normalizer to commit d80c3c484e1b80268f2b2dfaf7557fe65e34f321\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: sentencepiece!=0.1.92 in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 1)) (0.1.97)\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (3.20.3)\n",
      "Collecting protobuf\n",
      "  Using cached protobuf-4.21.12-cp37-abi3-manylinux2014_x86_64.whl (409 kB)\n",
      "Requirement already satisfied: datasets==1.11.0 in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 3)) (1.11.0)\n",
      "Requirement already satisfied: seqeval==1.2.2 in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 4)) (1.2.2)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets==1.11.0->-r requirements.txt (line 3)) (0.70.14)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets==1.11.0->-r requirements.txt (line 3)) (5.1.0)\n",
      "Requirement already satisfied: huggingface-hub<0.1.0 in /opt/conda/lib/python3.7/site-packages (from datasets==1.11.0->-r requirements.txt (line 3)) (0.0.19)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from datasets==1.11.0->-r requirements.txt (line 3)) (23.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets==1.11.0->-r requirements.txt (line 3)) (1.3.5)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets==1.11.0->-r requirements.txt (line 3)) (0.3.6)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets==1.11.0->-r requirements.txt (line 3)) (3.2.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets==1.11.0->-r requirements.txt (line 3)) (1.21.6)\n",
      "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets==1.11.0->-r requirements.txt (line 3)) (10.0.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets==1.11.0->-r requirements.txt (line 3)) (2.28.1)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from datasets==1.11.0->-r requirements.txt (line 3)) (2022.11.0)\n",
      "Requirement already satisfied: tqdm>=4.42 in /opt/conda/lib/python3.7/site-packages (from datasets==1.11.0->-r requirements.txt (line 3)) (4.42.1)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.7/site-packages (from seqeval==1.2.2->-r requirements.txt (line 4)) (0.22.1)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.7/site-packages (from normalizer==0.0.1->-r requirements.txt (line 5)) (2022.10.31)\n",
      "Requirement already satisfied: emoji==1.4.2 in /opt/conda/lib/python3.7/site-packages (from normalizer==0.0.1->-r requirements.txt (line 5)) (1.4.2)\n",
      "Requirement already satisfied: ftfy==6.0.3 in /opt/conda/lib/python3.7/site-packages (from normalizer==0.0.1->-r requirements.txt (line 5)) (6.0.3)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.7/site-packages (from ftfy==6.0.3->normalizer==0.0.1->-r requirements.txt (line 5)) (0.1.8)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<0.1.0->datasets==1.11.0->-r requirements.txt (line 3)) (3.0.12)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<0.1.0->datasets==1.11.0->-r requirements.txt (line 3)) (6.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<0.1.0->datasets==1.11.0->-r requirements.txt (line 3)) (4.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets==1.11.0->-r requirements.txt (line 3)) (2.8)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets==1.11.0->-r requirements.txt (line 3)) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets==1.11.0->-r requirements.txt (line 3)) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets==1.11.0->-r requirements.txt (line 3)) (1.26.13)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval==1.2.2->-r requirements.txt (line 4)) (1.4.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval==1.2.2->-r requirements.txt (line 4)) (0.14.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets==1.11.0->-r requirements.txt (line 3)) (3.11.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets==1.11.0->-r requirements.txt (line 3)) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets==1.11.0->-r requirements.txt (line 3)) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets==1.11.0->-r requirements.txt (line 3)) (1.14.0)\n",
      "Installing collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.3\n",
      "    Uninstalling protobuf-3.20.3:\n",
      "      Successfully uninstalled protobuf-3.20.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sagemaker 2.120.0 requires importlib-metadata<5.0,>=1.4.0, but you have importlib-metadata 5.1.0 which is incompatible.\n",
      "sagemaker 2.120.0 requires protobuf<4.0,>=3.1, but you have protobuf 4.21.12 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed protobuf-4.21.12\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!bash setup.sh "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md  question_answering  sequence_classification\ttoken_classification\n",
      "figs\t   requirements.txt    setup.sh\t\t\ttransformers\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/banglabert/token_classification\n"
     ]
    }
   ],
   "source": [
    "%cd token_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md    outputs\t    token_classification.py  wandb\n",
      "evaluate.sh  sample_inputs  trainer.sh\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘outputs’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyring is skipped due to an exception: 'keyring.backends'\n",
      "Collecting protobuf==3.20.*\n",
      "  Using cached protobuf-3.20.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "Installing collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.21.12\n",
      "    Uninstalling protobuf-4.21.12:\n",
      "      Successfully uninstalled protobuf-4.21.12\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sagemaker 2.120.0 requires importlib-metadata<5.0,>=1.4.0, but you have importlib-metadata 5.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed protobuf-3.20.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install protobuf==3.20.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyring is skipped due to an exception: 'keyring.backends'\n",
      "Requirement already satisfied: wandb in /opt/conda/lib/python3.7/site-packages (0.13.9)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (5.6.7)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from wandb) (4.4.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from wandb) (59.3.0)\n",
      "Requirement already satisfied: pathtools in /opt/conda/lib/python3.7/site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (2.28.1)\n",
      "Requirement already satisfied: setproctitle in /opt/conda/lib/python3.7/site-packages (from wandb) (1.3.2)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.7/site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (7.0)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (3.1.30)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (1.13.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from docker-pycreds>=0.4.0->wandb) (1.14.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb) (4.0.10)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (1.26.13)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Currently logged in as: rmswargo98 (nlphackathon23). Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
      "Aborted!\n"
     ]
    }
   ],
   "source": [
    "!wandb login --relogin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01/21/2023 07:24:47 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "01/21/2023 07:24:47 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=IntervalStrategy.EPOCH,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=2,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/root/data/outputs1/runs/Jan21_07-24-47_datascience-1-0-ml-g4dn-xlarge-94fad2f4401e538ca1255dfa1e84,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=30.0,\n",
      "output_dir=/root/data/outputs1,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=16,\n",
      "per_device_train_batch_size=16,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=outputs1,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=['wandb'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/root/data/outputs1,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.EPOCH,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.1,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.1,\n",
      ")\n",
      "01/21/2023 07:24:47 - WARNING - datasets.builder - Using custom data configuration default-1d7119f94afd0da1\n",
      "01/21/2023 07:24:47 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
      "01/21/2023 07:24:47 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-1d7119f94afd0da1/0.0.0\n",
      "01/21/2023 07:24:47 - WARNING - datasets.builder - Reusing dataset json (/root/.cache/huggingface/datasets/json/default-1d7119f94afd0da1/0.0.0)\n",
      "01/21/2023 07:24:47 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-1d7119f94afd0da1/0.0.0\n",
      "100%|████████████████████████████████████████████| 3/3 [00:00<00:00, 303.66it/s]\n",
      "01/21/2023 07:24:47 - WARNING - datasets.fingerprint - Parameter 'column_names'=set() of the transform datasets.arrow_dataset.Dataset.remove_columns couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "01/21/2023 07:24:47 - INFO - datasets.fingerprint - Parameter 'column_names'=set() of the transform datasets.arrow_dataset.Dataset.remove_columns couldn't be hashed properly, a random hash was used instead.\n",
      "01/21/2023 07:24:47 - INFO - datasets.fingerprint - Parameter 'column_names'=set() of the transform datasets.arrow_dataset.Dataset.remove_columns couldn't be hashed properly, a random hash was used instead.\n",
      "[INFO|configuration_utils.py:561] 2023-01-21 07:24:47,821 >> loading configuration file https://huggingface.co/csebuetnlp/banglabert/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/60928dc4b87f5881692890e6541e6538f91588d2ea40cbbbdc04cfb2cb83a6b1.2388211ba94f448fcf40aef3c9526142a8c2f2a8fb4fce8a3801462f51b2bab5\n",
      "[INFO|configuration_utils.py:598] 2023-01-21 07:24:47,822 >> Model config ElectraConfig {\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.11.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:561] 2023-01-21 07:24:48,051 >> loading configuration file https://huggingface.co/csebuetnlp/banglabert/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/60928dc4b87f5881692890e6541e6538f91588d2ea40cbbbdc04cfb2cb83a6b1.2388211ba94f448fcf40aef3c9526142a8c2f2a8fb4fce8a3801462f51b2bab5\n",
      "[INFO|configuration_utils.py:598] 2023-01-21 07:24:48,052 >> Model config ElectraConfig {\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.11.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1739] 2023-01-21 07:24:48,180 >> loading file https://huggingface.co/csebuetnlp/banglabert/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/65e95b847336b6bf69b37fdb8682a97e822799adcd9745dcf9bf44cfe4db1b9a.8f92ca2cf7e2eaa550b10c40331ae9bf0f2e40abe3b549f66a3d7f13bfc6de47\n",
      "[INFO|tokenization_utils_base.py:1739] 2023-01-21 07:24:48,180 >> loading file https://huggingface.co/csebuetnlp/banglabert/resolve/main/tokenizer.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1739] 2023-01-21 07:24:48,180 >> loading file https://huggingface.co/csebuetnlp/banglabert/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1739] 2023-01-21 07:24:48,180 >> loading file https://huggingface.co/csebuetnlp/banglabert/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/7820dfc553e8dfb8a1e82042b7d0d691c7a7cd1e30ed2974218f696e81c5f3b1.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "[INFO|tokenization_utils_base.py:1739] 2023-01-21 07:24:48,180 >> loading file https://huggingface.co/csebuetnlp/banglabert/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/76fa87a0ec9c34c9b15732bf7e06bced447feff46287b8e7d246a55d301784d7.b4f59cefeba4296760d2cf1037142788b96f2be40230bf6393d2fba714562485\n",
      "[INFO|configuration_utils.py:561] 2023-01-21 07:24:48,205 >> loading configuration file https://huggingface.co/csebuetnlp/banglabert/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/60928dc4b87f5881692890e6541e6538f91588d2ea40cbbbdc04cfb2cb83a6b1.2388211ba94f448fcf40aef3c9526142a8c2f2a8fb4fce8a3801462f51b2bab5\n",
      "[INFO|configuration_utils.py:598] 2023-01-21 07:24:48,206 >> Model config ElectraConfig {\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.11.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:561] 2023-01-21 07:24:48,263 >> loading configuration file https://huggingface.co/csebuetnlp/banglabert/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/60928dc4b87f5881692890e6541e6538f91588d2ea40cbbbdc04cfb2cb83a6b1.2388211ba94f448fcf40aef3c9526142a8c2f2a8fb4fce8a3801462f51b2bab5\n",
      "[INFO|configuration_utils.py:598] 2023-01-21 07:24:48,264 >> Model config ElectraConfig {\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.11.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:1279] 2023-01-21 07:24:48,321 >> loading weights file https://huggingface.co/csebuetnlp/banglabert/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/913ea71768a80ccdde3a9ab9a88cf2a93f37a52008896997655d0f63b0d0743a.8aaedac281b72dbb5296319c53be5a4e4a52339eded3f68d49201e140e221615\n",
      "[WARNING|modeling_utils.py:1516] 2023-01-21 07:24:49,685 >> Some weights of the model checkpoint at csebuetnlp/banglabert were not used when initializing ElectraForTokenClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:1527] 2023-01-21 07:24:49,686 >> Some weights of ElectraForTokenClassification were not initialized from the model checkpoint at csebuetnlp/banglabert and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "01/21/2023 07:24:49 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.normalize_example at 0x7f65703efe60> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n",
      "01/21/2023 07:24:49 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-1d7119f94afd0da1/0.0.0/cache-23b8c1e9392456de.arrow\n",
      "01/21/2023 07:24:49 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.normalize_example at 0x7f65703efe60> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n",
      "01/21/2023 07:24:49 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-1d7119f94afd0da1/0.0.0/cache-1a3d1fa7bc8960a9.arrow\n",
      "01/21/2023 07:24:49 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.normalize_example at 0x7f65703efe60> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n",
      "01/21/2023 07:24:49 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-1d7119f94afd0da1/0.0.0/cache-bd9c66b3ad3c2d6d.arrow\n",
      "01/21/2023 07:24:49 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.tokenize_and_align_labels at 0x7f6570429b90> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n",
      "01/21/2023 07:24:49 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-1d7119f94afd0da1/0.0.0/cache-8b9d2434e465e150.arrow\n",
      "01/21/2023 07:24:49 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.tokenize_and_align_labels at 0x7f6570429b90> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n",
      "01/21/2023 07:24:49 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-1d7119f94afd0da1/0.0.0/cache-972a846916419f82.arrow\n",
      "01/21/2023 07:24:49 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.tokenize_and_align_labels at 0x7f6570429b90> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n",
      "01/21/2023 07:24:49 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-1d7119f94afd0da1/0.0.0/cache-0822e8f36c031199.arrow\n",
      "01/21/2023 07:24:49 - INFO - __main__ - Sample 488 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'input_ids': [2, 7909, 410, 874, 3119, 17, 5400, 1045, 919, 3056, 2913, 863, 1875, 16, 11404, 2971, 903, 2210, 18425, 4390, 4136, 955, 205, 3], 'labels': [-100, 12, -100, 1, 7, -100, -100, -100, 12, 12, -100, -100, 12, -100, 12, 12, 12, 12, 12, -100, 12, 12, -100, -100], 'tags': ['O', 'B-CW', 'I-CW', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'tokens': ['ফিল্মে', 'দি', 'এক্স-ফাইল্\\u200cস', 'এর', 'অভিযোজনের', 'ক্ষেত্রে,', 'বৃহত্তর', 'প্রভাব', 'এবং', 'দীর্ঘ', 'প্লটলাইন', 'জড়িত', 'ছিল।']}.\n",
      "01/21/2023 07:24:49 - INFO - __main__ - Sample 1535 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'input_ids': [2, 1060, 6849, 1169, 924, 5446, 792, 16, 24998, 1873, 8346, 900, 990, 2867, 15412, 5427, 792, 205, 3], 'labels': [-100, 12, 12, 12, 12, 12, 12, -100, 12, 12, 12, 12, 12, 0, 6, 12, 12, -100, -100], 'tags': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CORP', 'I-CORP', 'O', 'O'], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'tokens': ['তাদের', 'কারখানা', 'বন্ধ', 'করতে', 'অস্বীকার', 'করে,', 'শ্রমিকরা', 'এটি', 'চালানোর', 'জন্য', 'একটি', 'শ্রমিক', 'সমবায়', 'স্থাপন', 'করে।']}.\n",
      "01/21/2023 07:24:49 - INFO - __main__ - Sample 3582 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'input_ids': [2, 26876, 7258, 3513, 16, 2630, 6125, 16, 2630, 903, 5638, 461, 2202, 1357, 792, 205, 3], 'labels': [-100, 12, 3, 9, 12, 12, -100, -100, 12, 12, 12, -100, 12, 12, 12, -100, -100], 'tags': ['O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'tokens': ['মৌমাছি', 'পৌর', 'এলাকা', ',', 'বনভূমি,', 'বন', 'এবং', 'হিথ', 'এলাকায়', 'বাস', 'করে।']}.\n",
      "[INFO|trainer.py:521] 2023-01-21 07:24:52,654 >> The following columns in the training set  don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tags, tokens.\n",
      "[INFO|trainer.py:1168] 2023-01-21 07:24:52,663 >> ***** Running training *****\n",
      "[INFO|trainer.py:1169] 2023-01-21 07:24:52,663 >>   Num examples = 16100\n",
      "[INFO|trainer.py:1170] 2023-01-21 07:24:52,663 >>   Num Epochs = 30\n",
      "[INFO|trainer.py:1171] 2023-01-21 07:24:52,663 >>   Instantaneous batch size per device = 16\n",
      "[INFO|trainer.py:1172] 2023-01-21 07:24:52,663 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1173] 2023-01-21 07:24:52,663 >>   Gradient Accumulation steps = 2\n",
      "[INFO|trainer.py:1174] 2023-01-21 07:24:52,663 >>   Total optimization steps = 15090\n",
      "[INFO|integrations.py:447] 2023-01-21 07:24:52,664 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrmswargo98\u001b[0m (\u001b[33mnlphackathon23\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/root/banglabert/token_classification/wandb/run-20230121_072452-a2knp41o\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m/root/data/outputs1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/nlphackathon23/huggingface\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/nlphackathon23/huggingface/runs/a2knp41o\u001b[0m\n",
      "{'loss': 1.0772, 'learning_rate': 6.666666666666667e-06, 'epoch': 1.0}          \n",
      "  3%|█▎                                     | 503/15090 [01:43<54:23,  4.47it/s][INFO|trainer.py:521] 2023-01-21 07:26:47,550 >> The following columns in the evaluation set  don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tags, tokens.\n",
      "[INFO|trainer.py:2181] 2023-01-21 07:26:47,553 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2183] 2023-01-21 07:26:47,553 >>   Num examples = 800\n",
      "[INFO|trainer.py:2186] 2023-01-21 07:26:47,553 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:00<00:01, 39.82it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:01, 37.31it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:01, 36.91it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:00<00:00, 36.66it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:00<00:00, 35.70it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:00<00:00, 35.00it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:00<00:00, 32.44it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:00<00:00, 31.96it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:01<00:00, 32.08it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:01<00:00, 31.84it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:01<00:00, 31.28it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.35286131501197815, 'eval_CORP_precision': 0.12980769230769232, 'eval_CORP_recall': 0.2125984251968504, 'eval_CORP_f1': 0.16119402985074627, 'eval_CORP_number': 127, 'eval_CW_precision': 0.2564102564102564, 'eval_CW_recall': 0.08333333333333333, 'eval_CW_f1': 0.12578616352201258, 'eval_CW_number': 120, 'eval_GRP_precision': 0.2190082644628099, 'eval_GRP_recall': 0.4491525423728814, 'eval_GRP_f1': 0.29444444444444445, 'eval_GRP_number': 118, 'eval_LOC_precision': 0.6428571428571429, 'eval_LOC_recall': 0.1782178217821782, 'eval_LOC_f1': 0.27906976744186046, 'eval_LOC_number': 101, 'eval_PER_precision': 0.7206703910614525, 'eval_PER_recall': 0.8958333333333334, 'eval_PER_f1': 0.7987616099071208, 'eval_PER_number': 144, 'eval_PROD_precision': 0.25925925925925924, 'eval_PROD_recall': 0.07368421052631578, 'eval_PROD_f1': 0.11475409836065571, 'eval_PROD_number': 190, 'eval_micro_avg_precision': 0.33466666666666667, 'eval_micro_avg_recall': 0.31375, 'eval_micro_avg_f1': 0.32387096774193547, 'eval_micro_avg_number': 800, 'eval_macro_avg_precision': 0.3713355010597689, 'eval_macro_avg_recall': 0.3154699444241488, 'eval_macro_avg_f1': 0.29566835225447335, 'eval_macro_avg_number': 800, 'eval_weighted_avg_precision': 0.3638276873744989, 'eval_weighted_avg_recall': 0.31375, 'eval_weighted_avg_f1': 0.2941517786061358, 'eval_weighted_avg_number': 800, 'eval_overall_accuracy': 0.9025452433949482, 'eval_runtime': 1.6967, 'eval_samples_per_second': 471.505, 'eval_steps_per_second': 29.469, 'epoch': 1.0}\n",
      "  3%|█▎                                     | 503/15090 [01:45<54:23,  4.47it/s]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 31.81it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:1935] 2023-01-21 07:26:49,259 >> Saving model checkpoint to /root/data/outputs1/checkpoint-503\n",
      "[INFO|configuration_utils.py:391] 2023-01-21 07:26:49,269 >> Configuration saved in /root/data/outputs1/checkpoint-503/config.json\n",
      "[INFO|modeling_utils.py:1001] 2023-01-21 07:26:53,787 >> Model weights saved in /root/data/outputs1/checkpoint-503/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2020] 2023-01-21 07:26:53,795 >> tokenizer config file saved in /root/data/outputs1/checkpoint-503/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2026] 2023-01-21 07:26:53,801 >> Special tokens file saved in /root/data/outputs1/checkpoint-503/special_tokens_map.json\n",
      "{'loss': 0.2958, 'learning_rate': 1.3333333333333333e-05, 'epoch': 2.0}         \n",
      "  7%|██▌                                   | 1006/15090 [03:43<50:50,  4.62it/s][INFO|trainer.py:521] 2023-01-21 07:28:47,626 >> The following columns in the evaluation set  don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tags, tokens.\n",
      "[INFO|trainer.py:2181] 2023-01-21 07:28:47,629 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2183] 2023-01-21 07:28:47,629 >>   Num examples = 800\n",
      "[INFO|trainer.py:2186] 2023-01-21 07:28:47,629 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:00<00:01, 41.36it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:01, 38.29it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:00, 37.26it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:00<00:00, 36.43it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:00<00:00, 35.53it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:00<00:00, 34.63it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:00<00:00, 31.99it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:00<00:00, 31.56it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:01<00:00, 31.64it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:01<00:00, 31.49it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:01<00:00, 30.51it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.1780719757080078, 'eval_CORP_precision': 0.5052083333333334, 'eval_CORP_recall': 0.7637795275590551, 'eval_CORP_f1': 0.6081504702194357, 'eval_CORP_number': 127, 'eval_CW_precision': 0.6705882352941176, 'eval_CW_recall': 0.475, 'eval_CW_f1': 0.5560975609756097, 'eval_CW_number': 120, 'eval_GRP_precision': 0.7008547008547008, 'eval_GRP_recall': 0.6949152542372882, 'eval_GRP_f1': 0.6978723404255319, 'eval_GRP_number': 118, 'eval_LOC_precision': 0.8255813953488372, 'eval_LOC_recall': 0.7029702970297029, 'eval_LOC_f1': 0.7593582887700535, 'eval_LOC_number': 101, 'eval_PER_precision': 0.9090909090909091, 'eval_PER_recall': 0.9027777777777778, 'eval_PER_f1': 0.9059233449477352, 'eval_PER_number': 144, 'eval_PROD_precision': 0.625, 'eval_PROD_recall': 0.5, 'eval_PROD_f1': 0.5555555555555556, 'eval_PROD_number': 190, 'eval_micro_avg_precision': 0.6864516129032258, 'eval_micro_avg_recall': 0.665, 'eval_micro_avg_f1': 0.6755555555555555, 'eval_micro_avg_number': 800, 'eval_macro_avg_precision': 0.706053928986983, 'eval_macro_avg_recall': 0.6732404761006373, 'eval_macro_avg_f1': 0.6804929268156537, 'eval_macro_avg_number': 800, 'eval_weighted_avg_precision': 0.700469641386007, 'eval_weighted_avg_recall': 0.665, 'eval_weighted_avg_f1': 0.6737743219986989, 'eval_weighted_avg_number': 800, 'eval_overall_accuracy': 0.9491919094164328, 'eval_runtime': 1.7279, 'eval_samples_per_second': 462.988, 'eval_steps_per_second': 28.937, 'epoch': 2.0}\n",
      "  7%|██▌                                   | 1006/15090 [03:45<50:50,  4.62it/s]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 30.83it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:1935] 2023-01-21 07:28:49,365 >> Saving model checkpoint to /root/data/outputs1/checkpoint-1006\n",
      "[INFO|configuration_utils.py:391] 2023-01-21 07:28:49,375 >> Configuration saved in /root/data/outputs1/checkpoint-1006/config.json\n",
      "[INFO|modeling_utils.py:1001] 2023-01-21 07:28:53,973 >> Model weights saved in /root/data/outputs1/checkpoint-1006/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2020] 2023-01-21 07:28:53,982 >> tokenizer config file saved in /root/data/outputs1/checkpoint-1006/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2026] 2023-01-21 07:28:53,990 >> Special tokens file saved in /root/data/outputs1/checkpoint-1006/special_tokens_map.json\n",
      "{'loss': 0.184, 'learning_rate': 2e-05, 'epoch': 3.0}                           \n",
      " 10%|███▊                                  | 1509/15090 [05:45<47:23,  4.78it/s][INFO|trainer.py:521] 2023-01-21 07:30:49,148 >> The following columns in the evaluation set  don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tags, tokens.\n",
      "[INFO|trainer.py:2181] 2023-01-21 07:30:49,151 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2183] 2023-01-21 07:30:49,151 >>   Num examples = 800\n",
      "[INFO|trainer.py:2186] 2023-01-21 07:30:49,151 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:00<00:01, 39.92it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:01, 37.12it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:01, 36.31it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:00<00:00, 35.38it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:00<00:00, 34.21it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:00<00:00, 33.83it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:00<00:00, 31.04it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:00<00:00, 30.50it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:01<00:00, 30.88it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:01<00:00, 30.40it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:01<00:00, 30.21it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:01<00:00, 29.70it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.10840073227882385, 'eval_CORP_precision': 0.751937984496124, 'eval_CORP_recall': 0.7637795275590551, 'eval_CORP_f1': 0.7578125, 'eval_CORP_number': 127, 'eval_CW_precision': 0.7142857142857143, 'eval_CW_recall': 0.6666666666666666, 'eval_CW_f1': 0.689655172413793, 'eval_CW_number': 120, 'eval_GRP_precision': 0.8446601941747572, 'eval_GRP_recall': 0.7372881355932204, 'eval_GRP_f1': 0.7873303167420815, 'eval_GRP_number': 118, 'eval_LOC_precision': 0.7909090909090909, 'eval_LOC_recall': 0.8613861386138614, 'eval_LOC_f1': 0.8246445497630333, 'eval_LOC_number': 101, 'eval_PER_precision': 0.9210526315789473, 'eval_PER_recall': 0.9722222222222222, 'eval_PER_f1': 0.9459459459459458, 'eval_PER_number': 144, 'eval_PROD_precision': 0.6979166666666666, 'eval_PROD_recall': 0.7052631578947368, 'eval_PROD_f1': 0.7015706806282722, 'eval_PROD_number': 190, 'eval_micro_avg_precision': 0.7832080200501254, 'eval_micro_avg_recall': 0.78125, 'eval_micro_avg_f1': 0.7822277847309137, 'eval_micro_avg_number': 800, 'eval_macro_avg_precision': 0.7867937136852169, 'eval_macro_avg_recall': 0.7844343080916271, 'eval_macro_avg_f1': 0.7844931942488543, 'eval_macro_avg_number': 800, 'eval_weighted_avg_precision': 0.7824973455672101, 'eval_weighted_avg_recall': 0.78125, 'eval_weighted_avg_f1': 0.7808869132835938, 'eval_weighted_avg_number': 800, 'eval_overall_accuracy': 0.968547372495887, 'eval_runtime': 1.7559, 'eval_samples_per_second': 455.619, 'eval_steps_per_second': 28.476, 'epoch': 3.0}\n",
      " 10%|███▊                                  | 1509/15090 [05:46<47:23,  4.78it/s]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 30.20it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:1935] 2023-01-21 07:30:50,914 >> Saving model checkpoint to /root/data/outputs1/checkpoint-1509\n",
      "[INFO|configuration_utils.py:391] 2023-01-21 07:30:50,925 >> Configuration saved in /root/data/outputs1/checkpoint-1509/config.json\n",
      "[INFO|modeling_utils.py:1001] 2023-01-21 07:30:55,460 >> Model weights saved in /root/data/outputs1/checkpoint-1509/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2020] 2023-01-21 07:30:55,469 >> tokenizer config file saved in /root/data/outputs1/checkpoint-1509/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2026] 2023-01-21 07:30:55,475 >> Special tokens file saved in /root/data/outputs1/checkpoint-1509/special_tokens_map.json\n",
      "{'loss': 0.1255, 'learning_rate': 1.925925925925926e-05, 'epoch': 4.0}          \n",
      " 13%|█████                                 | 2012/15090 [07:46<44:23,  4.91it/s][INFO|trainer.py:521] 2023-01-21 07:32:50,952 >> The following columns in the evaluation set  don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tags, tokens.\n",
      "[INFO|trainer.py:2181] 2023-01-21 07:32:50,954 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2183] 2023-01-21 07:32:50,954 >>   Num examples = 800\n",
      "[INFO|trainer.py:2186] 2023-01-21 07:32:50,954 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:00<00:01, 41.05it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:01, 37.69it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:01, 36.49it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:00<00:00, 35.76it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:00<00:00, 34.62it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:00<00:00, 34.08it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:00<00:00, 31.25it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:00<00:00, 30.56it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:01<00:00, 31.31it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:01<00:00, 30.73it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:01<00:00, 30.84it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.06925419718027115, 'eval_CORP_precision': 0.859504132231405, 'eval_CORP_recall': 0.8188976377952756, 'eval_CORP_f1': 0.8387096774193549, 'eval_CORP_number': 127, 'eval_CW_precision': 0.8290598290598291, 'eval_CW_recall': 0.8083333333333333, 'eval_CW_f1': 0.818565400843882, 'eval_CW_number': 120, 'eval_GRP_precision': 0.8737864077669902, 'eval_GRP_recall': 0.7627118644067796, 'eval_GRP_f1': 0.8144796380090498, 'eval_GRP_number': 118, 'eval_LOC_precision': 0.7981651376146789, 'eval_LOC_recall': 0.8613861386138614, 'eval_LOC_f1': 0.8285714285714286, 'eval_LOC_number': 101, 'eval_PER_precision': 0.9215686274509803, 'eval_PER_recall': 0.9791666666666666, 'eval_PER_f1': 0.9494949494949494, 'eval_PER_number': 144, 'eval_PROD_precision': 0.8349514563106796, 'eval_PROD_recall': 0.9052631578947369, 'eval_PROD_f1': 0.8686868686868687, 'eval_PROD_number': 190, 'eval_micro_avg_precision': 0.8541409147095179, 'eval_micro_avg_recall': 0.86375, 'eval_micro_avg_f1': 0.8589185829707892, 'eval_micro_avg_number': 800, 'eval_macro_avg_precision': 0.8528392650724271, 'eval_macro_avg_recall': 0.855959799785109, 'eval_macro_avg_f1': 0.8530846605042556, 'eval_macro_avg_number': 800, 'eval_weighted_avg_precision': 0.8546404229351569, 'eval_weighted_avg_recall': 0.86375, 'eval_weighted_avg_f1': 0.8578950831026049, 'eval_weighted_avg_number': 800, 'eval_overall_accuracy': 0.9805477596051485, 'eval_runtime': 1.7405, 'eval_samples_per_second': 459.649, 'eval_steps_per_second': 28.728, 'epoch': 4.0}\n",
      " 13%|█████                                 | 2012/15090 [07:48<44:23,  4.91it/s]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 30.32it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:1935] 2023-01-21 07:32:52,703 >> Saving model checkpoint to /root/data/outputs1/checkpoint-2012\n",
      "[INFO|configuration_utils.py:391] 2023-01-21 07:32:52,713 >> Configuration saved in /root/data/outputs1/checkpoint-2012/config.json\n",
      "[INFO|modeling_utils.py:1001] 2023-01-21 07:32:57,373 >> Model weights saved in /root/data/outputs1/checkpoint-2012/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2020] 2023-01-21 07:32:57,381 >> tokenizer config file saved in /root/data/outputs1/checkpoint-2012/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2026] 2023-01-21 07:32:57,388 >> Special tokens file saved in /root/data/outputs1/checkpoint-2012/special_tokens_map.json\n",
      "{'loss': 0.0862, 'learning_rate': 1.851851851851852e-05, 'epoch': 5.0}          \n",
      " 17%|██████▎                               | 2515/15090 [09:48<43:10,  4.85it/s][INFO|trainer.py:521] 2023-01-21 07:34:52,358 >> The following columns in the evaluation set  don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tags, tokens.\n",
      "[INFO|trainer.py:2181] 2023-01-21 07:34:52,360 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2183] 2023-01-21 07:34:52,361 >>   Num examples = 800\n",
      "[INFO|trainer.py:2186] 2023-01-21 07:34:52,361 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:00<00:01, 40.57it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:01, 37.77it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:01, 36.98it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:00<00:00, 36.35it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:00<00:00, 35.04it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:00<00:00, 34.41it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:00<00:00, 31.39it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:00<00:00, 30.58it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:01<00:00, 31.24it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:01<00:00, 30.73it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:01<00:00, 30.39it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:01<00:00, 29.74it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.04717768728733063, 'eval_CORP_precision': 0.8169014084507042, 'eval_CORP_recall': 0.9133858267716536, 'eval_CORP_f1': 0.862453531598513, 'eval_CORP_number': 127, 'eval_CW_precision': 0.9, 'eval_CW_recall': 0.825, 'eval_CW_f1': 0.8608695652173912, 'eval_CW_number': 120, 'eval_GRP_precision': 0.8888888888888888, 'eval_GRP_recall': 0.8135593220338984, 'eval_GRP_f1': 0.8495575221238938, 'eval_GRP_number': 118, 'eval_LOC_precision': 0.91, 'eval_LOC_recall': 0.900990099009901, 'eval_LOC_f1': 0.9054726368159204, 'eval_LOC_number': 101, 'eval_PER_precision': 0.9657534246575342, 'eval_PER_recall': 0.9791666666666666, 'eval_PER_f1': 0.9724137931034483, 'eval_PER_number': 144, 'eval_PROD_precision': 0.8774509803921569, 'eval_PROD_recall': 0.9421052631578948, 'eval_PROD_f1': 0.9086294416243655, 'eval_PROD_number': 190, 'eval_micro_avg_precision': 0.891358024691358, 'eval_micro_avg_recall': 0.9025, 'eval_micro_avg_f1': 0.8968944099378882, 'eval_micro_avg_number': 800, 'eval_macro_avg_precision': 0.8931657837315473, 'eval_macro_avg_recall': 0.8957011962733358, 'eval_macro_avg_f1': 0.8932327484139222, 'eval_macro_avg_number': 800, 'eval_weighted_avg_precision': 0.8929119339841539, 'eval_weighted_avg_recall': 0.9025, 'eval_weighted_avg_f1': 0.8965045629795644, 'eval_weighted_avg_number': 800, 'eval_overall_accuracy': 0.9870318397367657, 'eval_runtime': 1.7401, 'eval_samples_per_second': 459.741, 'eval_steps_per_second': 28.734, 'epoch': 5.0}\n",
      " 17%|██████▎                               | 2515/15090 [09:50<43:10,  4.85it/s]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 30.41it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:1935] 2023-01-21 07:34:54,108 >> Saving model checkpoint to /root/data/outputs1/checkpoint-2515\n",
      "[INFO|configuration_utils.py:391] 2023-01-21 07:34:54,119 >> Configuration saved in /root/data/outputs1/checkpoint-2515/config.json\n",
      "[INFO|modeling_utils.py:1001] 2023-01-21 07:34:58,749 >> Model weights saved in /root/data/outputs1/checkpoint-2515/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2020] 2023-01-21 07:34:58,757 >> tokenizer config file saved in /root/data/outputs1/checkpoint-2515/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2026] 2023-01-21 07:34:58,764 >> Special tokens file saved in /root/data/outputs1/checkpoint-2515/special_tokens_map.json\n",
      "{'loss': 0.0595, 'learning_rate': 1.7777777777777777e-05, 'epoch': 6.0}         \n",
      " 20%|███████▌                              | 3018/15090 [11:49<41:08,  4.89it/s][INFO|trainer.py:521] 2023-01-21 07:36:53,653 >> The following columns in the evaluation set  don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tags, tokens.\n",
      "[INFO|trainer.py:2181] 2023-01-21 07:36:53,656 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2183] 2023-01-21 07:36:53,656 >>   Num examples = 800\n",
      "[INFO|trainer.py:2186] 2023-01-21 07:36:53,656 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:00<00:01, 39.99it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:01, 37.42it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:01, 36.87it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:00<00:00, 36.13it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:00<00:00, 34.80it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:00<00:00, 34.20it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:00<00:00, 31.41it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:00<00:00, 30.55it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:01<00:00, 31.37it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:01<00:00, 30.74it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:01<00:00, 30.86it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.027209758758544922, 'eval_CORP_precision': 0.928, 'eval_CORP_recall': 0.9133858267716536, 'eval_CORP_f1': 0.9206349206349207, 'eval_CORP_number': 127, 'eval_CW_precision': 0.940677966101695, 'eval_CW_recall': 0.925, 'eval_CW_f1': 0.9327731092436976, 'eval_CW_number': 120, 'eval_GRP_precision': 0.8943089430894309, 'eval_GRP_recall': 0.9322033898305084, 'eval_GRP_f1': 0.9128630705394191, 'eval_GRP_number': 118, 'eval_LOC_precision': 0.8715596330275229, 'eval_LOC_recall': 0.9405940594059405, 'eval_LOC_f1': 0.9047619047619048, 'eval_LOC_number': 101, 'eval_PER_precision': 0.9794520547945206, 'eval_PER_recall': 0.9930555555555556, 'eval_PER_f1': 0.9862068965517242, 'eval_PER_number': 144, 'eval_PROD_precision': 0.9417989417989417, 'eval_PROD_recall': 0.9368421052631579, 'eval_PROD_f1': 0.9393139841688655, 'eval_PROD_number': 190, 'eval_micro_avg_precision': 0.9296296296296296, 'eval_micro_avg_recall': 0.94125, 'eval_micro_avg_f1': 0.9354037267080745, 'eval_micro_avg_number': 800, 'eval_macro_avg_precision': 0.9259662564686851, 'eval_macro_avg_recall': 0.9401801561378026, 'eval_macro_avg_f1': 0.9327589809834219, 'eval_macro_avg_number': 800, 'eval_weighted_avg_precision': 0.9303452862309325, 'eval_weighted_avg_recall': 0.94125, 'eval_weighted_avg_f1': 0.935544566037519, 'eval_weighted_avg_number': 800, 'eval_overall_accuracy': 0.9928384786606019, 'eval_runtime': 1.7342, 'eval_samples_per_second': 461.309, 'eval_steps_per_second': 28.832, 'epoch': 6.0}\n",
      " 20%|███████▌                              | 3018/15090 [11:51<41:08,  4.89it/s]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 30.32it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:1935] 2023-01-21 07:36:55,398 >> Saving model checkpoint to /root/data/outputs1/checkpoint-3018\n",
      "[INFO|configuration_utils.py:391] 2023-01-21 07:36:55,409 >> Configuration saved in /root/data/outputs1/checkpoint-3018/config.json\n",
      "[INFO|modeling_utils.py:1001] 2023-01-21 07:37:00,032 >> Model weights saved in /root/data/outputs1/checkpoint-3018/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2020] 2023-01-21 07:37:00,041 >> tokenizer config file saved in /root/data/outputs1/checkpoint-3018/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2026] 2023-01-21 07:37:00,047 >> Special tokens file saved in /root/data/outputs1/checkpoint-3018/special_tokens_map.json\n",
      "{'loss': 0.0422, 'learning_rate': 1.7037037037037038e-05, 'epoch': 7.0}         \n",
      " 23%|████████▊                             | 3521/15090 [13:51<41:28,  4.65it/s][INFO|trainer.py:521] 2023-01-21 07:38:55,853 >> The following columns in the evaluation set  don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tags, tokens.\n",
      "[INFO|trainer.py:2181] 2023-01-21 07:38:55,856 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2183] 2023-01-21 07:38:55,856 >>   Num examples = 800\n",
      "[INFO|trainer.py:2186] 2023-01-21 07:38:55,856 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:00<00:01, 39.06it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:01, 36.42it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:01, 35.74it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:00<00:00, 35.17it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:00<00:00, 33.86it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:00<00:00, 33.45it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:00<00:00, 30.78it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:00<00:00, 30.20it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:01<00:00, 30.71it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:01<00:00, 30.28it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:01<00:00, 30.40it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:01<00:00, 29.71it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.016758117824792862, 'eval_CORP_precision': 0.968503937007874, 'eval_CORP_recall': 0.968503937007874, 'eval_CORP_f1': 0.968503937007874, 'eval_CORP_number': 127, 'eval_CW_precision': 0.9829059829059829, 'eval_CW_recall': 0.9583333333333334, 'eval_CW_f1': 0.970464135021097, 'eval_CW_number': 120, 'eval_GRP_precision': 0.9736842105263158, 'eval_GRP_recall': 0.940677966101695, 'eval_GRP_f1': 0.956896551724138, 'eval_GRP_number': 118, 'eval_LOC_precision': 0.9320388349514563, 'eval_LOC_recall': 0.9504950495049505, 'eval_LOC_f1': 0.9411764705882353, 'eval_LOC_number': 101, 'eval_PER_precision': 0.9862068965517241, 'eval_PER_recall': 0.9930555555555556, 'eval_PER_f1': 0.9896193771626298, 'eval_PER_number': 144, 'eval_PROD_precision': 0.9432989690721649, 'eval_PROD_recall': 0.9631578947368421, 'eval_PROD_f1': 0.953125, 'eval_PROD_number': 190, 'eval_micro_avg_precision': 0.96375, 'eval_micro_avg_recall': 0.96375, 'eval_micro_avg_f1': 0.96375, 'eval_micro_avg_number': 800, 'eval_macro_avg_precision': 0.964439805169253, 'eval_macro_avg_recall': 0.9623706227067084, 'eval_macro_avg_f1': 0.9632975785839957, 'eval_macro_avg_number': 800, 'eval_weighted_avg_precision': 0.9640249679350998, 'eval_weighted_avg_recall': 0.96375, 'eval_weighted_avg_f1': 0.963784066433513, 'eval_weighted_avg_number': 800, 'eval_overall_accuracy': 0.9959353527533146, 'eval_runtime': 1.7586, 'eval_samples_per_second': 454.898, 'eval_steps_per_second': 28.431, 'epoch': 7.0}\n",
      " 23%|████████▊                             | 3521/15090 [13:53<41:28,  4.65it/s]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 30.28it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:1935] 2023-01-21 07:38:57,622 >> Saving model checkpoint to /root/data/outputs1/checkpoint-3521\n",
      "[INFO|configuration_utils.py:391] 2023-01-21 07:38:57,632 >> Configuration saved in /root/data/outputs1/checkpoint-3521/config.json\n",
      "[INFO|modeling_utils.py:1001] 2023-01-21 07:39:02,242 >> Model weights saved in /root/data/outputs1/checkpoint-3521/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2020] 2023-01-21 07:39:02,250 >> tokenizer config file saved in /root/data/outputs1/checkpoint-3521/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2026] 2023-01-21 07:39:02,260 >> Special tokens file saved in /root/data/outputs1/checkpoint-3521/special_tokens_map.json\n",
      "{'loss': 0.0306, 'learning_rate': 1.6296296296296297e-05, 'epoch': 8.0}         \n",
      " 27%|██████████▏                           | 4024/15090 [15:53<37:33,  4.91it/s][INFO|trainer.py:521] 2023-01-21 07:40:57,632 >> The following columns in the evaluation set  don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tags, tokens.\n",
      "[INFO|trainer.py:2181] 2023-01-21 07:40:57,635 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2183] 2023-01-21 07:40:57,635 >>   Num examples = 800\n",
      "[INFO|trainer.py:2186] 2023-01-21 07:40:57,635 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:00<00:01, 41.58it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:01, 38.35it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:00, 37.39it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:00<00:00, 36.53it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:00<00:00, 35.11it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:00<00:00, 34.40it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:00<00:00, 31.20it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:00<00:00, 30.67it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:01<00:00, 31.27it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:01<00:00, 30.67it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:01<00:00, 30.91it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.01327724102884531, 'eval_CORP_precision': 0.9612403100775194, 'eval_CORP_recall': 0.9763779527559056, 'eval_CORP_f1': 0.9687500000000001, 'eval_CORP_number': 127, 'eval_CW_precision': 1.0, 'eval_CW_recall': 0.9416666666666667, 'eval_CW_f1': 0.9699570815450643, 'eval_CW_number': 120, 'eval_GRP_precision': 0.9824561403508771, 'eval_GRP_recall': 0.9491525423728814, 'eval_GRP_f1': 0.9655172413793103, 'eval_GRP_number': 118, 'eval_LOC_precision': 0.9423076923076923, 'eval_LOC_recall': 0.9702970297029703, 'eval_LOC_f1': 0.9560975609756097, 'eval_LOC_number': 101, 'eval_PER_precision': 0.9863013698630136, 'eval_PER_recall': 1.0, 'eval_PER_f1': 0.993103448275862, 'eval_PER_number': 144, 'eval_PROD_precision': 0.9692307692307692, 'eval_PROD_recall': 0.9947368421052631, 'eval_PROD_f1': 0.9818181818181819, 'eval_PROD_number': 190, 'eval_micro_avg_precision': 0.9737827715355806, 'eval_micro_avg_recall': 0.975, 'eval_micro_avg_f1': 0.9743910056214865, 'eval_micro_avg_number': 800, 'eval_macro_avg_precision': 0.9735893803049785, 'eval_macro_avg_recall': 0.9720385056006146, 'eval_macro_avg_f1': 0.9725405856656714, 'eval_macro_avg_number': 800, 'eval_weighted_avg_precision': 0.9742020803480568, 'eval_weighted_avg_recall': 0.975, 'eval_weighted_avg_f1': 0.9743441737798519, 'eval_weighted_avg_number': 800, 'eval_overall_accuracy': 0.9969031259072874, 'eval_runtime': 1.7362, 'eval_samples_per_second': 460.778, 'eval_steps_per_second': 28.799, 'epoch': 8.0}\n",
      " 27%|██████████▏                           | 4024/15090 [15:55<37:33,  4.91it/s]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 30.25it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:1935] 2023-01-21 07:40:59,379 >> Saving model checkpoint to /root/data/outputs1/checkpoint-4024\n",
      "[INFO|configuration_utils.py:391] 2023-01-21 07:40:59,388 >> Configuration saved in /root/data/outputs1/checkpoint-4024/config.json\n",
      "[INFO|modeling_utils.py:1001] 2023-01-21 07:41:03,930 >> Model weights saved in /root/data/outputs1/checkpoint-4024/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2020] 2023-01-21 07:41:03,940 >> tokenizer config file saved in /root/data/outputs1/checkpoint-4024/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2026] 2023-01-21 07:41:03,948 >> Special tokens file saved in /root/data/outputs1/checkpoint-4024/special_tokens_map.json\n",
      "{'loss': 0.0234, 'learning_rate': 1.555555555555556e-05, 'epoch': 9.0}          \n",
      " 30%|███████████▍                          | 4527/15090 [17:55<37:47,  4.66it/s][INFO|trainer.py:521] 2023-01-21 07:42:59,251 >> The following columns in the evaluation set  don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tags, tokens.\n",
      "[INFO|trainer.py:2181] 2023-01-21 07:42:59,254 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2183] 2023-01-21 07:42:59,254 >>   Num examples = 800\n",
      "[INFO|trainer.py:2186] 2023-01-21 07:42:59,254 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:00<00:01, 40.77it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:01, 37.72it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:01, 36.78it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:00<00:00, 36.17it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:00<00:00, 34.66it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:00<00:00, 33.97it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:00<00:00, 31.20it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:00<00:00, 30.64it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:01<00:00, 31.29it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:01<00:00, 30.82it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:01<00:00, 30.55it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:01<00:00, 29.87it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.007538957521319389, 'eval_CORP_precision': 0.9920634920634921, 'eval_CORP_recall': 0.984251968503937, 'eval_CORP_f1': 0.9881422924901185, 'eval_CORP_number': 127, 'eval_CW_precision': 0.9831932773109243, 'eval_CW_recall': 0.975, 'eval_CW_f1': 0.9790794979079498, 'eval_CW_number': 120, 'eval_GRP_precision': 1.0, 'eval_GRP_recall': 0.9745762711864406, 'eval_GRP_f1': 0.9871244635193133, 'eval_GRP_number': 118, 'eval_LOC_precision': 0.9607843137254902, 'eval_LOC_recall': 0.9702970297029703, 'eval_LOC_f1': 0.9655172413793103, 'eval_LOC_number': 101, 'eval_PER_precision': 0.9863013698630136, 'eval_PER_recall': 1.0, 'eval_PER_f1': 0.993103448275862, 'eval_PER_number': 144, 'eval_PROD_precision': 0.9895287958115183, 'eval_PROD_recall': 0.9947368421052631, 'eval_PROD_f1': 0.9921259842519685, 'eval_PROD_number': 190, 'eval_micro_avg_precision': 0.986232790988736, 'eval_micro_avg_recall': 0.985, 'eval_micro_avg_f1': 0.985616010006254, 'eval_micro_avg_number': 800, 'eval_macro_avg_precision': 0.9853118747957398, 'eval_macro_avg_recall': 0.9831436852497686, 'eval_macro_avg_f1': 0.9841821546374204, 'eval_macro_avg_number': 800, 'eval_weighted_avg_precision': 0.9863154261501392, 'eval_weighted_avg_recall': 0.985, 'eval_weighted_avg_f1': 0.9856154656617331, 'eval_weighted_avg_number': 800, 'eval_overall_accuracy': 0.9985483402690409, 'eval_runtime': 1.7411, 'eval_samples_per_second': 459.491, 'eval_steps_per_second': 28.718, 'epoch': 9.0}\n",
      " 30%|███████████▍                          | 4527/15090 [17:56<37:47,  4.66it/s]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 30.49it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:1935] 2023-01-21 07:43:01,004 >> Saving model checkpoint to /root/data/outputs1/checkpoint-4527\n",
      "[INFO|configuration_utils.py:391] 2023-01-21 07:43:01,013 >> Configuration saved in /root/data/outputs1/checkpoint-4527/config.json\n",
      "[INFO|modeling_utils.py:1001] 2023-01-21 07:43:05,625 >> Model weights saved in /root/data/outputs1/checkpoint-4527/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2020] 2023-01-21 07:43:05,634 >> tokenizer config file saved in /root/data/outputs1/checkpoint-4527/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2026] 2023-01-21 07:43:05,641 >> Special tokens file saved in /root/data/outputs1/checkpoint-4527/special_tokens_map.json\n",
      "{'loss': 0.0172, 'learning_rate': 1.4814814814814815e-05, 'epoch': 10.0}        \n",
      " 33%|████████████▋                         | 5030/15090 [19:56<35:56,  4.66it/s][INFO|trainer.py:521] 2023-01-21 07:45:00,447 >> The following columns in the evaluation set  don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tags, tokens.\n",
      "[INFO|trainer.py:2181] 2023-01-21 07:45:00,449 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2183] 2023-01-21 07:45:00,449 >>   Num examples = 800\n",
      "[INFO|trainer.py:2186] 2023-01-21 07:45:00,449 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:00<00:01, 40.54it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:01, 37.46it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:01, 36.70it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:00<00:00, 35.42it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:00<00:00, 34.21it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:00<00:00, 33.80it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:00<00:00, 30.56it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:00<00:00, 30.08it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:01<00:00, 30.75it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:01<00:00, 30.47it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:01<00:00, 30.14it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:01<00:00, 29.51it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.00613170862197876, 'eval_CORP_precision': 1.0, 'eval_CORP_recall': 0.9921259842519685, 'eval_CORP_f1': 0.9960474308300395, 'eval_CORP_number': 127, 'eval_CW_precision': 0.9831932773109243, 'eval_CW_recall': 0.975, 'eval_CW_f1': 0.9790794979079498, 'eval_CW_number': 120, 'eval_GRP_precision': 1.0, 'eval_GRP_recall': 0.9745762711864406, 'eval_GRP_f1': 0.9871244635193133, 'eval_GRP_number': 118, 'eval_LOC_precision': 0.9611650485436893, 'eval_LOC_recall': 0.9801980198019802, 'eval_LOC_f1': 0.9705882352941178, 'eval_LOC_number': 101, 'eval_PER_precision': 0.9795918367346939, 'eval_PER_recall': 1.0, 'eval_PER_f1': 0.9896907216494846, 'eval_PER_number': 144, 'eval_PROD_precision': 0.9947368421052631, 'eval_PROD_recall': 0.9947368421052631, 'eval_PROD_f1': 0.9947368421052631, 'eval_PROD_number': 190, 'eval_micro_avg_precision': 0.9875, 'eval_micro_avg_recall': 0.9875, 'eval_micro_avg_f1': 0.9875, 'eval_micro_avg_number': 800, 'eval_macro_avg_precision': 0.9864478341157619, 'eval_macro_avg_recall': 0.9861061862242755, 'eval_macro_avg_f1': 0.986211198551028, 'eval_macro_avg_number': 800, 'eval_weighted_avg_precision': 0.9876526095875243, 'eval_weighted_avg_recall': 0.9875, 'eval_weighted_avg_f1': 0.9875164073023496, 'eval_weighted_avg_number': 800, 'eval_overall_accuracy': 0.9988386722152327, 'eval_runtime': 1.7621, 'eval_samples_per_second': 453.995, 'eval_steps_per_second': 28.375, 'epoch': 10.0}\n",
      " 33%|████████████▋                         | 5030/15090 [19:58<35:56,  4.66it/s]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 30.09it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:1935] 2023-01-21 07:45:02,219 >> Saving model checkpoint to /root/data/outputs1/checkpoint-5030\n",
      "[INFO|configuration_utils.py:391] 2023-01-21 07:45:02,230 >> Configuration saved in /root/data/outputs1/checkpoint-5030/config.json\n",
      "[INFO|modeling_utils.py:1001] 2023-01-21 07:45:06,851 >> Model weights saved in /root/data/outputs1/checkpoint-5030/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2020] 2023-01-21 07:45:06,859 >> tokenizer config file saved in /root/data/outputs1/checkpoint-5030/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2026] 2023-01-21 07:45:06,866 >> Special tokens file saved in /root/data/outputs1/checkpoint-5030/special_tokens_map.json\n",
      "{'loss': 0.0144, 'learning_rate': 1.4074074074074075e-05, 'epoch': 11.0}        \n",
      " 37%|█████████████▉                        | 5533/15090 [21:58<33:02,  4.82it/s][INFO|trainer.py:521] 2023-01-21 07:47:02,460 >> The following columns in the evaluation set  don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tags, tokens.\n",
      "[INFO|trainer.py:2181] 2023-01-21 07:47:02,463 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2183] 2023-01-21 07:47:02,463 >>   Num examples = 800\n",
      "[INFO|trainer.py:2186] 2023-01-21 07:47:02,463 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:00<00:01, 40.22it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:01, 37.64it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:01, 36.67it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:00<00:00, 35.82it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:00<00:00, 34.72it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:00<00:00, 34.16it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:00<00:00, 31.06it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:00<00:00, 30.47it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:01<00:00, 31.04it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:01<00:00, 30.68it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:01<00:00, 30.65it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:01<00:00, 30.18it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.002327436115592718, 'eval_CORP_precision': 0.9921875, 'eval_CORP_recall': 1.0, 'eval_CORP_f1': 0.996078431372549, 'eval_CORP_number': 127, 'eval_CW_precision': 0.9915966386554622, 'eval_CW_recall': 0.9833333333333333, 'eval_CW_f1': 0.9874476987447698, 'eval_CW_number': 120, 'eval_GRP_precision': 1.0, 'eval_GRP_recall': 0.9830508474576272, 'eval_GRP_f1': 0.9914529914529915, 'eval_GRP_number': 118, 'eval_LOC_precision': 0.9805825242718447, 'eval_LOC_recall': 1.0, 'eval_LOC_f1': 0.9901960784313726, 'eval_LOC_number': 101, 'eval_PER_precision': 1.0, 'eval_PER_recall': 1.0, 'eval_PER_f1': 1.0, 'eval_PER_number': 144, 'eval_PROD_precision': 0.9947368421052631, 'eval_PROD_recall': 0.9947368421052631, 'eval_PROD_f1': 0.9947368421052631, 'eval_PROD_number': 190, 'eval_micro_avg_precision': 0.99375, 'eval_micro_avg_recall': 0.99375, 'eval_micro_avg_f1': 0.99375, 'eval_micro_avg_number': 800, 'eval_macro_avg_precision': 0.9931839175054283, 'eval_macro_avg_recall': 0.993520170482704, 'eval_macro_avg_f1': 0.9933186736844911, 'eval_macro_avg_number': 800, 'eval_weighted_avg_precision': 0.9937978051126396, 'eval_weighted_avg_recall': 0.99375, 'eval_weighted_avg_f1': 0.9937461769333846, 'eval_weighted_avg_number': 800, 'eval_overall_accuracy': 0.9994193361076164, 'eval_runtime': 1.7535, 'eval_samples_per_second': 456.234, 'eval_steps_per_second': 28.515, 'epoch': 11.0}\n",
      " 37%|█████████████▉                        | 5533/15090 [22:00<33:02,  4.82it/s]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 30.41it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:1935] 2023-01-21 07:47:04,225 >> Saving model checkpoint to /root/data/outputs1/checkpoint-5533\n",
      "[INFO|configuration_utils.py:391] 2023-01-21 07:47:04,235 >> Configuration saved in /root/data/outputs1/checkpoint-5533/config.json\n",
      "[INFO|modeling_utils.py:1001] 2023-01-21 07:47:08,847 >> Model weights saved in /root/data/outputs1/checkpoint-5533/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2020] 2023-01-21 07:47:08,855 >> tokenizer config file saved in /root/data/outputs1/checkpoint-5533/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2026] 2023-01-21 07:47:08,863 >> Special tokens file saved in /root/data/outputs1/checkpoint-5533/special_tokens_map.json\n",
      "{'loss': 0.0116, 'learning_rate': 1.3333333333333333e-05, 'epoch': 12.0}        \n",
      " 40%|███████████████▏                      | 6036/15090 [24:00<30:42,  4.91it/s][INFO|trainer.py:521] 2023-01-21 07:49:04,201 >> The following columns in the evaluation set  don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tags, tokens.\n",
      "[INFO|trainer.py:2181] 2023-01-21 07:49:04,204 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2183] 2023-01-21 07:49:04,204 >>   Num examples = 800\n",
      "[INFO|trainer.py:2186] 2023-01-21 07:49:04,204 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:00<00:01, 39.38it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:01, 36.71it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:01, 36.32it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:00<00:00, 35.39it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:00<00:00, 34.36it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:00<00:00, 33.85it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:00<00:00, 30.60it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:00<00:00, 29.97it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:01<00:00, 30.78it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:01<00:00, 30.21it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:01<00:00, 30.08it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:01<00:00, 29.34it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.0032696097623556852, 'eval_CORP_precision': 0.9844961240310077, 'eval_CORP_recall': 1.0, 'eval_CORP_f1': 0.9921875, 'eval_CORP_number': 127, 'eval_CW_precision': 0.9915966386554622, 'eval_CW_recall': 0.9833333333333333, 'eval_CW_f1': 0.9874476987447698, 'eval_CW_number': 120, 'eval_GRP_precision': 1.0, 'eval_GRP_recall': 0.9745762711864406, 'eval_GRP_f1': 0.9871244635193133, 'eval_GRP_number': 118, 'eval_LOC_precision': 1.0, 'eval_LOC_recall': 1.0, 'eval_LOC_f1': 1.0, 'eval_LOC_number': 101, 'eval_PER_precision': 0.9863013698630136, 'eval_PER_recall': 1.0, 'eval_PER_f1': 0.993103448275862, 'eval_PER_number': 144, 'eval_PROD_precision': 0.9947368421052631, 'eval_PROD_recall': 0.9947368421052631, 'eval_PROD_f1': 0.9947368421052631, 'eval_PROD_number': 190, 'eval_micro_avg_precision': 0.9925, 'eval_micro_avg_recall': 0.9925, 'eval_micro_avg_f1': 0.9925, 'eval_micro_avg_number': 800, 'eval_macro_avg_precision': 0.9928551624424579, 'eval_macro_avg_recall': 0.9921077411041729, 'eval_macro_avg_f1': 0.9924333254408682, 'eval_macro_avg_number': 800, 'eval_weighted_avg_precision': 0.9925625020635843, 'eval_weighted_avg_recall': 0.9925, 'eval_weighted_avg_f1': 0.9924863994954695, 'eval_weighted_avg_number': 800, 'eval_overall_accuracy': 0.9993225587922191, 'eval_runtime': 1.7594, 'eval_samples_per_second': 454.698, 'eval_steps_per_second': 28.419, 'epoch': 12.0}\n",
      " 40%|███████████████▏                      | 6036/15090 [24:01<30:42,  4.91it/s]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 30.10it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:1935] 2023-01-21 07:49:05,972 >> Saving model checkpoint to /root/data/outputs1/checkpoint-6036\n",
      "[INFO|configuration_utils.py:391] 2023-01-21 07:49:05,981 >> Configuration saved in /root/data/outputs1/checkpoint-6036/config.json\n",
      "[INFO|modeling_utils.py:1001] 2023-01-21 07:49:10,596 >> Model weights saved in /root/data/outputs1/checkpoint-6036/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2020] 2023-01-21 07:49:10,604 >> tokenizer config file saved in /root/data/outputs1/checkpoint-6036/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2026] 2023-01-21 07:49:10,611 >> Special tokens file saved in /root/data/outputs1/checkpoint-6036/special_tokens_map.json\n",
      "{'loss': 0.0096, 'learning_rate': 1.2592592592592593e-05, 'epoch': 13.0}        \n",
      " 43%|████████████████▍                     | 6539/15090 [26:01<29:52,  4.77it/s][INFO|trainer.py:521] 2023-01-21 07:51:05,782 >> The following columns in the evaluation set  don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tags, tokens.\n",
      "[INFO|trainer.py:2181] 2023-01-21 07:51:05,785 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2183] 2023-01-21 07:51:05,785 >>   Num examples = 800\n",
      "[INFO|trainer.py:2186] 2023-01-21 07:51:05,785 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:00<00:01, 39.11it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:01, 36.95it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:01, 36.32it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:00<00:00, 35.25it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:00<00:00, 34.41it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:00<00:00, 33.79it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:00<00:00, 30.63it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:00<00:00, 30.15it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:01<00:00, 30.79it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:01<00:00, 30.35it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:01<00:00, 30.24it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:01<00:00, 30.06it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.001557667157612741, 'eval_CORP_precision': 0.9921875, 'eval_CORP_recall': 1.0, 'eval_CORP_f1': 0.996078431372549, 'eval_CORP_number': 127, 'eval_CW_precision': 1.0, 'eval_CW_recall': 1.0, 'eval_CW_f1': 1.0, 'eval_CW_number': 120, 'eval_GRP_precision': 1.0, 'eval_GRP_recall': 0.9915254237288136, 'eval_GRP_f1': 0.9957446808510638, 'eval_GRP_number': 118, 'eval_LOC_precision': 1.0, 'eval_LOC_recall': 1.0, 'eval_LOC_f1': 1.0, 'eval_LOC_number': 101, 'eval_PER_precision': 1.0, 'eval_PER_recall': 1.0, 'eval_PER_f1': 1.0, 'eval_PER_number': 144, 'eval_PROD_precision': 1.0, 'eval_PROD_recall': 1.0, 'eval_PROD_f1': 1.0, 'eval_PROD_number': 190, 'eval_micro_avg_precision': 0.99875, 'eval_micro_avg_recall': 0.99875, 'eval_micro_avg_f1': 0.99875, 'eval_micro_avg_number': 800, 'eval_macro_avg_precision': 0.9986979166666666, 'eval_macro_avg_recall': 0.9985875706214689, 'eval_macro_avg_f1': 0.9986371853706021, 'eval_macro_avg_number': 800, 'eval_weighted_avg_precision': 0.998759765625, 'eval_weighted_avg_recall': 0.99875, 'eval_weighted_avg_f1': 0.9987497914059241, 'eval_weighted_avg_number': 800, 'eval_overall_accuracy': 0.9997096680538082, 'eval_runtime': 1.7588, 'eval_samples_per_second': 454.859, 'eval_steps_per_second': 28.429, 'epoch': 13.0}\n",
      " 43%|████████████████▍                     | 6539/15090 [26:03<29:52,  4.77it/s]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 29.72it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:1935] 2023-01-21 07:51:07,552 >> Saving model checkpoint to /root/data/outputs1/checkpoint-6539\n",
      "[INFO|configuration_utils.py:391] 2023-01-21 07:51:07,563 >> Configuration saved in /root/data/outputs1/checkpoint-6539/config.json\n",
      "[INFO|modeling_utils.py:1001] 2023-01-21 07:51:12,174 >> Model weights saved in /root/data/outputs1/checkpoint-6539/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2020] 2023-01-21 07:51:12,183 >> tokenizer config file saved in /root/data/outputs1/checkpoint-6539/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2026] 2023-01-21 07:51:12,190 >> Special tokens file saved in /root/data/outputs1/checkpoint-6539/special_tokens_map.json\n",
      "{'loss': 0.0073, 'learning_rate': 1.1851851851851852e-05, 'epoch': 14.0}        \n",
      " 47%|█████████████████▋                    | 7042/15090 [28:02<27:42,  4.84it/s][INFO|trainer.py:521] 2023-01-21 07:53:06,974 >> The following columns in the evaluation set  don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tags, tokens.\n",
      "[INFO|trainer.py:2181] 2023-01-21 07:53:06,976 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2183] 2023-01-21 07:53:06,976 >>   Num examples = 800\n",
      "[INFO|trainer.py:2186] 2023-01-21 07:53:06,976 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:00<00:01, 41.92it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:01, 38.55it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:00, 37.62it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:00<00:00, 36.30it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:00<00:00, 35.28it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:00<00:00, 34.46it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:00<00:00, 31.18it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:00<00:00, 30.68it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:01<00:00, 31.02it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:01<00:00, 30.55it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:01<00:00, 30.93it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.0005656426656059921, 'eval_CORP_precision': 0.9921875, 'eval_CORP_recall': 1.0, 'eval_CORP_f1': 0.996078431372549, 'eval_CORP_number': 127, 'eval_CW_precision': 1.0, 'eval_CW_recall': 0.9916666666666667, 'eval_CW_f1': 0.99581589958159, 'eval_CW_number': 120, 'eval_GRP_precision': 1.0, 'eval_GRP_recall': 1.0, 'eval_GRP_f1': 1.0, 'eval_GRP_number': 118, 'eval_LOC_precision': 1.0, 'eval_LOC_recall': 1.0, 'eval_LOC_f1': 1.0, 'eval_LOC_number': 101, 'eval_PER_precision': 1.0, 'eval_PER_recall': 1.0, 'eval_PER_f1': 1.0, 'eval_PER_number': 144, 'eval_PROD_precision': 1.0, 'eval_PROD_recall': 1.0, 'eval_PROD_f1': 1.0, 'eval_PROD_number': 190, 'eval_micro_avg_precision': 0.99875, 'eval_micro_avg_recall': 0.99875, 'eval_micro_avg_f1': 0.99875, 'eval_micro_avg_number': 800, 'eval_macro_avg_precision': 0.9986979166666666, 'eval_macro_avg_recall': 0.9986111111111112, 'eval_macro_avg_f1': 0.9986490551590231, 'eval_macro_avg_number': 800, 'eval_weighted_avg_precision': 0.998759765625, 'eval_weighted_avg_recall': 0.99875, 'eval_weighted_avg_f1': 0.9987498359176307, 'eval_weighted_avg_number': 800, 'eval_overall_accuracy': 0.9999032226846027, 'eval_runtime': 1.7354, 'eval_samples_per_second': 460.991, 'eval_steps_per_second': 28.812, 'epoch': 14.0}\n",
      " 47%|█████████████████▋                    | 7042/15090 [28:04<27:42,  4.84it/s]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 30.57it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:1935] 2023-01-21 07:53:08,719 >> Saving model checkpoint to /root/data/outputs1/checkpoint-7042\n",
      "[INFO|configuration_utils.py:391] 2023-01-21 07:53:08,729 >> Configuration saved in /root/data/outputs1/checkpoint-7042/config.json\n",
      "[INFO|modeling_utils.py:1001] 2023-01-21 07:53:13,353 >> Model weights saved in /root/data/outputs1/checkpoint-7042/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2020] 2023-01-21 07:53:13,361 >> tokenizer config file saved in /root/data/outputs1/checkpoint-7042/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2026] 2023-01-21 07:53:13,367 >> Special tokens file saved in /root/data/outputs1/checkpoint-7042/special_tokens_map.json\n",
      "{'loss': 0.0066, 'learning_rate': 1.1111111111111113e-05, 'epoch': 15.0}        \n",
      " 50%|███████████████████                   | 7545/15090 [30:04<26:05,  4.82it/s][INFO|trainer.py:521] 2023-01-21 07:55:08,712 >> The following columns in the evaluation set  don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tags, tokens.\n",
      "[INFO|trainer.py:2181] 2023-01-21 07:55:08,715 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2183] 2023-01-21 07:55:08,715 >>   Num examples = 800\n",
      "[INFO|trainer.py:2186] 2023-01-21 07:55:08,715 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:00<00:01, 39.88it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:01, 37.34it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:01, 36.90it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:00<00:00, 35.82it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:00<00:00, 34.75it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:00<00:00, 34.16it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:00<00:00, 31.09it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:00<00:00, 30.66it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:01<00:00, 31.25it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:01<00:00, 30.88it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:01<00:00, 30.96it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.000548925600014627, 'eval_CORP_precision': 1.0, 'eval_CORP_recall': 1.0, 'eval_CORP_f1': 1.0, 'eval_CORP_number': 127, 'eval_CW_precision': 0.9916666666666667, 'eval_CW_recall': 0.9916666666666667, 'eval_CW_f1': 0.9916666666666667, 'eval_CW_number': 120, 'eval_GRP_precision': 1.0, 'eval_GRP_recall': 1.0, 'eval_GRP_f1': 1.0, 'eval_GRP_number': 118, 'eval_LOC_precision': 1.0, 'eval_LOC_recall': 1.0, 'eval_LOC_f1': 1.0, 'eval_LOC_number': 101, 'eval_PER_precision': 1.0, 'eval_PER_recall': 1.0, 'eval_PER_f1': 1.0, 'eval_PER_number': 144, 'eval_PROD_precision': 1.0, 'eval_PROD_recall': 1.0, 'eval_PROD_f1': 1.0, 'eval_PROD_number': 190, 'eval_micro_avg_precision': 0.99875, 'eval_micro_avg_recall': 0.99875, 'eval_micro_avg_f1': 0.99875, 'eval_micro_avg_number': 800, 'eval_macro_avg_precision': 0.9986111111111112, 'eval_macro_avg_recall': 0.9986111111111112, 'eval_macro_avg_f1': 0.9986111111111112, 'eval_macro_avg_number': 800, 'eval_weighted_avg_precision': 0.99875, 'eval_weighted_avg_recall': 0.99875, 'eval_weighted_avg_f1': 0.99875, 'eval_weighted_avg_number': 800, 'eval_overall_accuracy': 0.9999032226846027, 'eval_runtime': 1.7395, 'eval_samples_per_second': 459.897, 'eval_steps_per_second': 28.744, 'epoch': 15.0}\n",
      " 50%|███████████████████                   | 7545/15090 [30:06<26:05,  4.82it/s]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 30.52it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:1935] 2023-01-21 07:55:10,463 >> Saving model checkpoint to /root/data/outputs1/checkpoint-7545\n",
      "[INFO|configuration_utils.py:391] 2023-01-21 07:55:10,473 >> Configuration saved in /root/data/outputs1/checkpoint-7545/config.json\n",
      "[INFO|modeling_utils.py:1001] 2023-01-21 07:55:15,082 >> Model weights saved in /root/data/outputs1/checkpoint-7545/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2020] 2023-01-21 07:55:15,090 >> tokenizer config file saved in /root/data/outputs1/checkpoint-7545/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2026] 2023-01-21 07:55:15,097 >> Special tokens file saved in /root/data/outputs1/checkpoint-7545/special_tokens_map.json\n",
      "{'loss': 0.0057, 'learning_rate': 1.037037037037037e-05, 'epoch': 16.0}         \n",
      " 53%|████████████████████▎                 | 8048/15090 [32:06<23:45,  4.94it/s][INFO|trainer.py:521] 2023-01-21 07:57:10,326 >> The following columns in the evaluation set  don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tags, tokens.\n",
      "[INFO|trainer.py:2181] 2023-01-21 07:57:10,328 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2183] 2023-01-21 07:57:10,328 >>   Num examples = 800\n",
      "[INFO|trainer.py:2186] 2023-01-21 07:57:10,329 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:00<00:01, 40.00it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:01, 37.27it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:01, 36.67it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:00<00:00, 35.95it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:00<00:00, 34.75it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:00<00:00, 34.14it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:00<00:00, 31.17it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:00<00:00, 30.64it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:01<00:00, 31.39it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:01<00:00, 30.74it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:01<00:00, 30.94it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.0007437847671099007, 'eval_CORP_precision': 0.9921875, 'eval_CORP_recall': 1.0, 'eval_CORP_f1': 0.996078431372549, 'eval_CORP_number': 127, 'eval_CW_precision': 1.0, 'eval_CW_recall': 0.9833333333333333, 'eval_CW_f1': 0.9915966386554621, 'eval_CW_number': 120, 'eval_GRP_precision': 1.0, 'eval_GRP_recall': 1.0, 'eval_GRP_f1': 1.0, 'eval_GRP_number': 118, 'eval_LOC_precision': 1.0, 'eval_LOC_recall': 1.0, 'eval_LOC_f1': 1.0, 'eval_LOC_number': 101, 'eval_PER_precision': 0.993103448275862, 'eval_PER_recall': 1.0, 'eval_PER_f1': 0.9965397923875432, 'eval_PER_number': 144, 'eval_PROD_precision': 1.0, 'eval_PROD_recall': 1.0, 'eval_PROD_f1': 1.0, 'eval_PROD_number': 190, 'eval_micro_avg_precision': 0.9975, 'eval_micro_avg_recall': 0.9975, 'eval_micro_avg_f1': 0.9975, 'eval_micro_avg_number': 800, 'eval_macro_avg_precision': 0.9975484913793103, 'eval_macro_avg_recall': 0.9972222222222222, 'eval_macro_avg_f1': 0.9973691437359258, 'eval_macro_avg_number': 800, 'eval_weighted_avg_precision': 0.9975183863146552, 'eval_weighted_avg_recall': 0.9975, 'eval_weighted_avg_f1': 0.9974941094084693, 'eval_weighted_avg_number': 800, 'eval_overall_accuracy': 0.9998064453692055, 'eval_runtime': 1.739, 'eval_samples_per_second': 460.04, 'eval_steps_per_second': 28.752, 'epoch': 16.0}\n",
      " 53%|████████████████████▎                 | 8048/15090 [32:07<23:45,  4.94it/s]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 30.36it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:1935] 2023-01-21 07:57:12,075 >> Saving model checkpoint to /root/data/outputs1/checkpoint-8048\n",
      "[INFO|configuration_utils.py:391] 2023-01-21 07:57:12,085 >> Configuration saved in /root/data/outputs1/checkpoint-8048/config.json\n",
      "[INFO|modeling_utils.py:1001] 2023-01-21 07:57:16,688 >> Model weights saved in /root/data/outputs1/checkpoint-8048/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2020] 2023-01-21 07:57:16,697 >> tokenizer config file saved in /root/data/outputs1/checkpoint-8048/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2026] 2023-01-21 07:57:16,703 >> Special tokens file saved in /root/data/outputs1/checkpoint-8048/special_tokens_map.json\n",
      "{'loss': 0.0046, 'learning_rate': 9.62962962962963e-06, 'epoch': 17.0}          \n",
      " 57%|█████████████████████▌                | 8551/15090 [34:07<22:21,  4.88it/s][INFO|trainer.py:521] 2023-01-21 07:59:11,362 >> The following columns in the evaluation set  don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tags, tokens.\n",
      "[INFO|trainer.py:2181] 2023-01-21 07:59:11,366 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2183] 2023-01-21 07:59:11,366 >>   Num examples = 800\n",
      "[INFO|trainer.py:2186] 2023-01-21 07:59:11,366 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:00<00:01, 40.65it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:01, 37.65it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:01, 36.79it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:00<00:00, 35.96it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:00<00:00, 34.52it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:00<00:00, 33.97it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:00<00:00, 30.84it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:00<00:00, 30.40it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:01<00:00, 31.18it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:01<00:00, 30.70it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:01<00:00, 30.48it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:01<00:00, 30.24it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.0003218829515390098, 'eval_CORP_precision': 1.0, 'eval_CORP_recall': 1.0, 'eval_CORP_f1': 1.0, 'eval_CORP_number': 127, 'eval_CW_precision': 1.0, 'eval_CW_recall': 1.0, 'eval_CW_f1': 1.0, 'eval_CW_number': 120, 'eval_GRP_precision': 1.0, 'eval_GRP_recall': 1.0, 'eval_GRP_f1': 1.0, 'eval_GRP_number': 118, 'eval_LOC_precision': 1.0, 'eval_LOC_recall': 1.0, 'eval_LOC_f1': 1.0, 'eval_LOC_number': 101, 'eval_PER_precision': 1.0, 'eval_PER_recall': 1.0, 'eval_PER_f1': 1.0, 'eval_PER_number': 144, 'eval_PROD_precision': 1.0, 'eval_PROD_recall': 1.0, 'eval_PROD_f1': 1.0, 'eval_PROD_number': 190, 'eval_micro_avg_precision': 1.0, 'eval_micro_avg_recall': 1.0, 'eval_micro_avg_f1': 1.0, 'eval_micro_avg_number': 800, 'eval_macro_avg_precision': 1.0, 'eval_macro_avg_recall': 1.0, 'eval_macro_avg_f1': 1.0, 'eval_macro_avg_number': 800, 'eval_weighted_avg_precision': 1.0, 'eval_weighted_avg_recall': 1.0, 'eval_weighted_avg_f1': 1.0, 'eval_weighted_avg_number': 800, 'eval_overall_accuracy': 1.0, 'eval_runtime': 1.7509, 'eval_samples_per_second': 456.905, 'eval_steps_per_second': 28.557, 'epoch': 17.0}\n",
      " 57%|█████████████████████▌                | 8551/15090 [34:09<22:21,  4.88it/s]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 29.84it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:1935] 2023-01-21 07:59:13,125 >> Saving model checkpoint to /root/data/outputs1/checkpoint-8551\n",
      "[INFO|configuration_utils.py:391] 2023-01-21 07:59:13,136 >> Configuration saved in /root/data/outputs1/checkpoint-8551/config.json\n",
      "[INFO|modeling_utils.py:1001] 2023-01-21 07:59:17,779 >> Model weights saved in /root/data/outputs1/checkpoint-8551/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2020] 2023-01-21 07:59:17,788 >> tokenizer config file saved in /root/data/outputs1/checkpoint-8551/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2026] 2023-01-21 07:59:17,795 >> Special tokens file saved in /root/data/outputs1/checkpoint-8551/special_tokens_map.json\n",
      "{'loss': 0.0038, 'learning_rate': 8.888888888888888e-06, 'epoch': 18.0}         \n",
      " 60%|██████████████████████▊               | 9054/15090 [36:08<21:46,  4.62it/s][INFO|trainer.py:521] 2023-01-21 08:01:13,024 >> The following columns in the evaluation set  don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tags, tokens.\n",
      "[INFO|trainer.py:2181] 2023-01-21 08:01:13,026 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2183] 2023-01-21 08:01:13,026 >>   Num examples = 800\n",
      "[INFO|trainer.py:2186] 2023-01-21 08:01:13,026 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:00<00:01, 40.44it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:01, 37.12it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:01, 36.30it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:00<00:00, 35.02it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:00<00:00, 34.17it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:00<00:00, 33.64it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:00<00:00, 30.38it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:00<00:00, 29.92it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:01<00:00, 30.65it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:01<00:00, 30.21it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:01<00:00, 30.11it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:01<00:00, 29.33it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.0001984768023248762, 'eval_CORP_precision': 1.0, 'eval_CORP_recall': 1.0, 'eval_CORP_f1': 1.0, 'eval_CORP_number': 127, 'eval_CW_precision': 1.0, 'eval_CW_recall': 1.0, 'eval_CW_f1': 1.0, 'eval_CW_number': 120, 'eval_GRP_precision': 1.0, 'eval_GRP_recall': 1.0, 'eval_GRP_f1': 1.0, 'eval_GRP_number': 118, 'eval_LOC_precision': 1.0, 'eval_LOC_recall': 1.0, 'eval_LOC_f1': 1.0, 'eval_LOC_number': 101, 'eval_PER_precision': 1.0, 'eval_PER_recall': 1.0, 'eval_PER_f1': 1.0, 'eval_PER_number': 144, 'eval_PROD_precision': 1.0, 'eval_PROD_recall': 1.0, 'eval_PROD_f1': 1.0, 'eval_PROD_number': 190, 'eval_micro_avg_precision': 1.0, 'eval_micro_avg_recall': 1.0, 'eval_micro_avg_f1': 1.0, 'eval_micro_avg_number': 800, 'eval_macro_avg_precision': 1.0, 'eval_macro_avg_recall': 1.0, 'eval_macro_avg_f1': 1.0, 'eval_macro_avg_number': 800, 'eval_weighted_avg_precision': 1.0, 'eval_weighted_avg_recall': 1.0, 'eval_weighted_avg_f1': 1.0, 'eval_weighted_avg_number': 800, 'eval_overall_accuracy': 1.0, 'eval_runtime': 1.7691, 'eval_samples_per_second': 452.195, 'eval_steps_per_second': 28.262, 'epoch': 18.0}\n",
      " 60%|██████████████████████▊               | 9054/15090 [36:10<21:46,  4.62it/s]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 29.90it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:1935] 2023-01-21 08:01:14,803 >> Saving model checkpoint to /root/data/outputs1/checkpoint-9054\n",
      "[INFO|configuration_utils.py:391] 2023-01-21 08:01:14,813 >> Configuration saved in /root/data/outputs1/checkpoint-9054/config.json\n",
      "[INFO|modeling_utils.py:1001] 2023-01-21 08:01:19,419 >> Model weights saved in /root/data/outputs1/checkpoint-9054/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2020] 2023-01-21 08:01:19,427 >> tokenizer config file saved in /root/data/outputs1/checkpoint-9054/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2026] 2023-01-21 08:01:19,435 >> Special tokens file saved in /root/data/outputs1/checkpoint-9054/special_tokens_map.json\n",
      "{'loss': 0.0038, 'learning_rate': 8.148148148148148e-06, 'epoch': 19.0}         \n",
      " 63%|████████████████████████              | 9557/15090 [38:11<19:24,  4.75it/s][INFO|trainer.py:521] 2023-01-21 08:03:15,230 >> The following columns in the evaluation set  don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tags, tokens.\n",
      "[INFO|trainer.py:2181] 2023-01-21 08:03:15,234 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2183] 2023-01-21 08:03:15,234 >>   Num examples = 800\n",
      "[INFO|trainer.py:2186] 2023-01-21 08:03:15,234 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:00<00:01, 39.86it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:01, 37.28it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:01, 36.47it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:00<00:00, 35.38it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:00<00:00, 34.14it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:00<00:00, 33.68it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:00<00:00, 30.37it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:00<00:00, 29.67it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:01<00:00, 30.48it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:01<00:00, 30.17it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:01<00:00, 30.22it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:01<00:00, 29.53it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.00019641144899651408, 'eval_CORP_precision': 1.0, 'eval_CORP_recall': 1.0, 'eval_CORP_f1': 1.0, 'eval_CORP_number': 127, 'eval_CW_precision': 1.0, 'eval_CW_recall': 1.0, 'eval_CW_f1': 1.0, 'eval_CW_number': 120, 'eval_GRP_precision': 1.0, 'eval_GRP_recall': 1.0, 'eval_GRP_f1': 1.0, 'eval_GRP_number': 118, 'eval_LOC_precision': 1.0, 'eval_LOC_recall': 1.0, 'eval_LOC_f1': 1.0, 'eval_LOC_number': 101, 'eval_PER_precision': 1.0, 'eval_PER_recall': 1.0, 'eval_PER_f1': 1.0, 'eval_PER_number': 144, 'eval_PROD_precision': 1.0, 'eval_PROD_recall': 1.0, 'eval_PROD_f1': 1.0, 'eval_PROD_number': 190, 'eval_micro_avg_precision': 1.0, 'eval_micro_avg_recall': 1.0, 'eval_micro_avg_f1': 1.0, 'eval_micro_avg_number': 800, 'eval_macro_avg_precision': 1.0, 'eval_macro_avg_recall': 1.0, 'eval_macro_avg_f1': 1.0, 'eval_macro_avg_number': 800, 'eval_weighted_avg_precision': 1.0, 'eval_weighted_avg_recall': 1.0, 'eval_weighted_avg_f1': 1.0, 'eval_weighted_avg_number': 800, 'eval_overall_accuracy': 1.0, 'eval_runtime': 1.7628, 'eval_samples_per_second': 453.836, 'eval_steps_per_second': 28.365, 'epoch': 19.0}\n",
      " 63%|████████████████████████              | 9557/15090 [38:12<19:24,  4.75it/s]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 30.14it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:1935] 2023-01-21 08:03:17,003 >> Saving model checkpoint to /root/data/outputs1/checkpoint-9557\n",
      "[INFO|configuration_utils.py:391] 2023-01-21 08:03:17,017 >> Configuration saved in /root/data/outputs1/checkpoint-9557/config.json\n",
      "[INFO|modeling_utils.py:1001] 2023-01-21 08:03:21,624 >> Model weights saved in /root/data/outputs1/checkpoint-9557/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2020] 2023-01-21 08:03:21,633 >> tokenizer config file saved in /root/data/outputs1/checkpoint-9557/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2026] 2023-01-21 08:03:21,639 >> Special tokens file saved in /root/data/outputs1/checkpoint-9557/special_tokens_map.json\n",
      "{'loss': 0.0031, 'learning_rate': 7.4074074074074075e-06, 'epoch': 20.0}        \n",
      " 67%|████████████████████████▋            | 10060/15090 [40:12<18:31,  4.52it/s][INFO|trainer.py:521] 2023-01-21 08:05:16,663 >> The following columns in the evaluation set  don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tags, tokens.\n",
      "[INFO|trainer.py:2181] 2023-01-21 08:05:16,666 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2183] 2023-01-21 08:05:16,666 >>   Num examples = 800\n",
      "[INFO|trainer.py:2186] 2023-01-21 08:05:16,666 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:00<00:01, 40.14it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:01, 37.27it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:01, 36.40it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:00<00:00, 35.44it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:00<00:00, 34.46it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:00<00:00, 33.79it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:00<00:00, 30.72it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:00<00:00, 30.24it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:01<00:00, 30.79it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:01<00:00, 30.41it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:01<00:00, 30.81it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.00014847055717837065, 'eval_CORP_precision': 1.0, 'eval_CORP_recall': 1.0, 'eval_CORP_f1': 1.0, 'eval_CORP_number': 127, 'eval_CW_precision': 1.0, 'eval_CW_recall': 1.0, 'eval_CW_f1': 1.0, 'eval_CW_number': 120, 'eval_GRP_precision': 1.0, 'eval_GRP_recall': 1.0, 'eval_GRP_f1': 1.0, 'eval_GRP_number': 118, 'eval_LOC_precision': 1.0, 'eval_LOC_recall': 1.0, 'eval_LOC_f1': 1.0, 'eval_LOC_number': 101, 'eval_PER_precision': 1.0, 'eval_PER_recall': 1.0, 'eval_PER_f1': 1.0, 'eval_PER_number': 144, 'eval_PROD_precision': 1.0, 'eval_PROD_recall': 1.0, 'eval_PROD_f1': 1.0, 'eval_PROD_number': 190, 'eval_micro_avg_precision': 1.0, 'eval_micro_avg_recall': 1.0, 'eval_micro_avg_f1': 1.0, 'eval_micro_avg_number': 800, 'eval_macro_avg_precision': 1.0, 'eval_macro_avg_recall': 1.0, 'eval_macro_avg_f1': 1.0, 'eval_macro_avg_number': 800, 'eval_weighted_avg_precision': 1.0, 'eval_weighted_avg_recall': 1.0, 'eval_weighted_avg_f1': 1.0, 'eval_weighted_avg_number': 800, 'eval_overall_accuracy': 1.0, 'eval_runtime': 1.7451, 'eval_samples_per_second': 458.429, 'eval_steps_per_second': 28.652, 'epoch': 20.0}\n",
      " 67%|████████████████████████▋            | 10060/15090 [40:14<18:31,  4.52it/s]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 30.43it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:1935] 2023-01-21 08:05:18,418 >> Saving model checkpoint to /root/data/outputs1/checkpoint-10060\n",
      "[INFO|configuration_utils.py:391] 2023-01-21 08:05:18,428 >> Configuration saved in /root/data/outputs1/checkpoint-10060/config.json\n",
      "[INFO|modeling_utils.py:1001] 2023-01-21 08:05:23,028 >> Model weights saved in /root/data/outputs1/checkpoint-10060/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2020] 2023-01-21 08:05:23,037 >> tokenizer config file saved in /root/data/outputs1/checkpoint-10060/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2026] 2023-01-21 08:05:23,044 >> Special tokens file saved in /root/data/outputs1/checkpoint-10060/special_tokens_map.json\n",
      "{'loss': 0.0028, 'learning_rate': 6.666666666666667e-06, 'epoch': 21.0}         \n",
      " 70%|█████████████████████████▉           | 10563/15090 [42:13<15:45,  4.79it/s][INFO|trainer.py:521] 2023-01-21 08:07:17,998 >> The following columns in the evaluation set  don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tags, tokens.\n",
      "[INFO|trainer.py:2181] 2023-01-21 08:07:18,000 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2183] 2023-01-21 08:07:18,001 >>   Num examples = 800\n",
      "[INFO|trainer.py:2186] 2023-01-21 08:07:18,001 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:00<00:01, 40.90it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:01, 37.89it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:01, 36.97it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:00<00:00, 36.35it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:00<00:00, 35.11it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:00<00:00, 34.27it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:00<00:00, 31.16it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:00<00:00, 30.42it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:01<00:00, 30.96it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:01<00:00, 30.58it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:01<00:00, 30.87it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.00013587126159109175, 'eval_CORP_precision': 1.0, 'eval_CORP_recall': 1.0, 'eval_CORP_f1': 1.0, 'eval_CORP_number': 127, 'eval_CW_precision': 1.0, 'eval_CW_recall': 1.0, 'eval_CW_f1': 1.0, 'eval_CW_number': 120, 'eval_GRP_precision': 1.0, 'eval_GRP_recall': 1.0, 'eval_GRP_f1': 1.0, 'eval_GRP_number': 118, 'eval_LOC_precision': 1.0, 'eval_LOC_recall': 1.0, 'eval_LOC_f1': 1.0, 'eval_LOC_number': 101, 'eval_PER_precision': 1.0, 'eval_PER_recall': 1.0, 'eval_PER_f1': 1.0, 'eval_PER_number': 144, 'eval_PROD_precision': 1.0, 'eval_PROD_recall': 1.0, 'eval_PROD_f1': 1.0, 'eval_PROD_number': 190, 'eval_micro_avg_precision': 1.0, 'eval_micro_avg_recall': 1.0, 'eval_micro_avg_f1': 1.0, 'eval_micro_avg_number': 800, 'eval_macro_avg_precision': 1.0, 'eval_macro_avg_recall': 1.0, 'eval_macro_avg_f1': 1.0, 'eval_macro_avg_number': 800, 'eval_weighted_avg_precision': 1.0, 'eval_weighted_avg_recall': 1.0, 'eval_weighted_avg_f1': 1.0, 'eval_weighted_avg_number': 800, 'eval_overall_accuracy': 1.0, 'eval_runtime': 1.7371, 'eval_samples_per_second': 460.544, 'eval_steps_per_second': 28.784, 'epoch': 21.0}\n",
      " 70%|█████████████████████████▉           | 10563/15090 [42:15<15:45,  4.79it/s]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 30.39it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:1935] 2023-01-21 08:07:19,746 >> Saving model checkpoint to /root/data/outputs1/checkpoint-10563\n",
      "[INFO|configuration_utils.py:391] 2023-01-21 08:07:19,756 >> Configuration saved in /root/data/outputs1/checkpoint-10563/config.json\n",
      "[INFO|modeling_utils.py:1001] 2023-01-21 08:07:24,383 >> Model weights saved in /root/data/outputs1/checkpoint-10563/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2020] 2023-01-21 08:07:24,391 >> tokenizer config file saved in /root/data/outputs1/checkpoint-10563/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2026] 2023-01-21 08:07:24,398 >> Special tokens file saved in /root/data/outputs1/checkpoint-10563/special_tokens_map.json\n",
      "{'loss': 0.0024, 'learning_rate': 5.925925925925926e-06, 'epoch': 22.0}         \n",
      " 73%|███████████████████████████▏         | 11066/15090 [44:15<14:38,  4.58it/s][INFO|trainer.py:521] 2023-01-21 08:09:19,619 >> The following columns in the evaluation set  don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tags, tokens.\n",
      "[INFO|trainer.py:2181] 2023-01-21 08:09:19,621 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2183] 2023-01-21 08:09:19,622 >>   Num examples = 800\n",
      "[INFO|trainer.py:2186] 2023-01-21 08:09:19,622 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:00<00:01, 40.27it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:01, 37.68it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:01, 36.74it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:00<00:00, 35.73it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:00<00:00, 34.55it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:00<00:00, 33.89it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:00<00:00, 30.71it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:00<00:00, 30.46it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:01<00:00, 30.95it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:01<00:00, 30.49it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:01<00:00, 30.74it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.0001289981446461752, 'eval_CORP_precision': 1.0, 'eval_CORP_recall': 1.0, 'eval_CORP_f1': 1.0, 'eval_CORP_number': 127, 'eval_CW_precision': 1.0, 'eval_CW_recall': 1.0, 'eval_CW_f1': 1.0, 'eval_CW_number': 120, 'eval_GRP_precision': 1.0, 'eval_GRP_recall': 1.0, 'eval_GRP_f1': 1.0, 'eval_GRP_number': 118, 'eval_LOC_precision': 1.0, 'eval_LOC_recall': 1.0, 'eval_LOC_f1': 1.0, 'eval_LOC_number': 101, 'eval_PER_precision': 1.0, 'eval_PER_recall': 1.0, 'eval_PER_f1': 1.0, 'eval_PER_number': 144, 'eval_PROD_precision': 1.0, 'eval_PROD_recall': 1.0, 'eval_PROD_f1': 1.0, 'eval_PROD_number': 190, 'eval_micro_avg_precision': 1.0, 'eval_micro_avg_recall': 1.0, 'eval_micro_avg_f1': 1.0, 'eval_micro_avg_number': 800, 'eval_macro_avg_precision': 1.0, 'eval_macro_avg_recall': 1.0, 'eval_macro_avg_f1': 1.0, 'eval_macro_avg_number': 800, 'eval_weighted_avg_precision': 1.0, 'eval_weighted_avg_recall': 1.0, 'eval_weighted_avg_f1': 1.0, 'eval_weighted_avg_number': 800, 'eval_overall_accuracy': 1.0, 'eval_runtime': 1.7466, 'eval_samples_per_second': 458.033, 'eval_steps_per_second': 28.627, 'epoch': 22.0}\n",
      " 73%|███████████████████████████▏         | 11066/15090 [44:17<14:38,  4.58it/s]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 30.22it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:1935] 2023-01-21 08:09:21,376 >> Saving model checkpoint to /root/data/outputs1/checkpoint-11066\n",
      "[INFO|configuration_utils.py:391] 2023-01-21 08:09:21,386 >> Configuration saved in /root/data/outputs1/checkpoint-11066/config.json\n",
      "[INFO|modeling_utils.py:1001] 2023-01-21 08:09:25,992 >> Model weights saved in /root/data/outputs1/checkpoint-11066/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2020] 2023-01-21 08:09:26,002 >> tokenizer config file saved in /root/data/outputs1/checkpoint-11066/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2026] 2023-01-21 08:09:26,008 >> Special tokens file saved in /root/data/outputs1/checkpoint-11066/special_tokens_map.json\n",
      "{'loss': 0.0021, 'learning_rate': 5.185185185185185e-06, 'epoch': 23.0}         \n",
      " 77%|████████████████████████████▎        | 11569/15090 [46:17<12:14,  4.79it/s][INFO|trainer.py:521] 2023-01-21 08:11:21,711 >> The following columns in the evaluation set  don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tags, tokens.\n",
      "[INFO|trainer.py:2181] 2023-01-21 08:11:21,714 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2183] 2023-01-21 08:11:21,714 >>   Num examples = 800\n",
      "[INFO|trainer.py:2186] 2023-01-21 08:11:21,714 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:00<00:01, 40.51it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:01, 37.38it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:01, 36.47it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:00<00:00, 35.57it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:00<00:00, 34.39it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:00<00:00, 33.75it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:00<00:00, 30.94it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:00<00:00, 30.15it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:01<00:00, 30.74it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:01<00:00, 30.41it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:01<00:00, 30.28it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:01<00:00, 29.82it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.00011289017857052386, 'eval_CORP_precision': 1.0, 'eval_CORP_recall': 1.0, 'eval_CORP_f1': 1.0, 'eval_CORP_number': 127, 'eval_CW_precision': 1.0, 'eval_CW_recall': 1.0, 'eval_CW_f1': 1.0, 'eval_CW_number': 120, 'eval_GRP_precision': 1.0, 'eval_GRP_recall': 1.0, 'eval_GRP_f1': 1.0, 'eval_GRP_number': 118, 'eval_LOC_precision': 1.0, 'eval_LOC_recall': 1.0, 'eval_LOC_f1': 1.0, 'eval_LOC_number': 101, 'eval_PER_precision': 1.0, 'eval_PER_recall': 1.0, 'eval_PER_f1': 1.0, 'eval_PER_number': 144, 'eval_PROD_precision': 1.0, 'eval_PROD_recall': 1.0, 'eval_PROD_f1': 1.0, 'eval_PROD_number': 190, 'eval_micro_avg_precision': 1.0, 'eval_micro_avg_recall': 1.0, 'eval_micro_avg_f1': 1.0, 'eval_micro_avg_number': 800, 'eval_macro_avg_precision': 1.0, 'eval_macro_avg_recall': 1.0, 'eval_macro_avg_f1': 1.0, 'eval_macro_avg_number': 800, 'eval_weighted_avg_precision': 1.0, 'eval_weighted_avg_recall': 1.0, 'eval_weighted_avg_f1': 1.0, 'eval_weighted_avg_number': 800, 'eval_overall_accuracy': 1.0, 'eval_runtime': 1.7613, 'eval_samples_per_second': 454.214, 'eval_steps_per_second': 28.388, 'epoch': 23.0}\n",
      " 77%|████████████████████████████▎        | 11569/15090 [46:19<12:14,  4.79it/s]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 30.28it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:1935] 2023-01-21 08:11:23,483 >> Saving model checkpoint to /root/data/outputs1/checkpoint-11569\n",
      "[INFO|configuration_utils.py:391] 2023-01-21 08:11:23,494 >> Configuration saved in /root/data/outputs1/checkpoint-11569/config.json\n",
      "[INFO|modeling_utils.py:1001] 2023-01-21 08:11:28,102 >> Model weights saved in /root/data/outputs1/checkpoint-11569/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2020] 2023-01-21 08:11:28,111 >> tokenizer config file saved in /root/data/outputs1/checkpoint-11569/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2026] 2023-01-21 08:11:28,117 >> Special tokens file saved in /root/data/outputs1/checkpoint-11569/special_tokens_map.json\n",
      "{'loss': 0.0018, 'learning_rate': 4.444444444444444e-06, 'epoch': 24.0}         \n",
      " 80%|█████████████████████████████▌       | 12072/15090 [48:19<10:17,  4.89it/s][INFO|trainer.py:521] 2023-01-21 08:13:23,700 >> The following columns in the evaluation set  don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tags, tokens.\n",
      "[INFO|trainer.py:2181] 2023-01-21 08:13:23,704 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2183] 2023-01-21 08:13:23,704 >>   Num examples = 800\n",
      "[INFO|trainer.py:2186] 2023-01-21 08:13:23,704 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:00<00:01, 40.77it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:01, 37.93it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:00, 37.27it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:00<00:00, 36.04it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:00<00:00, 34.98it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:00<00:00, 34.15it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:00<00:00, 30.70it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:00<00:00, 30.38it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:01<00:00, 30.85it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:01<00:00, 30.14it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:01<00:00, 30.15it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:01<00:00, 29.38it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.00010230862244497985, 'eval_CORP_precision': 1.0, 'eval_CORP_recall': 1.0, 'eval_CORP_f1': 1.0, 'eval_CORP_number': 127, 'eval_CW_precision': 1.0, 'eval_CW_recall': 1.0, 'eval_CW_f1': 1.0, 'eval_CW_number': 120, 'eval_GRP_precision': 1.0, 'eval_GRP_recall': 1.0, 'eval_GRP_f1': 1.0, 'eval_GRP_number': 118, 'eval_LOC_precision': 1.0, 'eval_LOC_recall': 1.0, 'eval_LOC_f1': 1.0, 'eval_LOC_number': 101, 'eval_PER_precision': 1.0, 'eval_PER_recall': 1.0, 'eval_PER_f1': 1.0, 'eval_PER_number': 144, 'eval_PROD_precision': 1.0, 'eval_PROD_recall': 1.0, 'eval_PROD_f1': 1.0, 'eval_PROD_number': 190, 'eval_micro_avg_precision': 1.0, 'eval_micro_avg_recall': 1.0, 'eval_micro_avg_f1': 1.0, 'eval_micro_avg_number': 800, 'eval_macro_avg_precision': 1.0, 'eval_macro_avg_recall': 1.0, 'eval_macro_avg_f1': 1.0, 'eval_macro_avg_number': 800, 'eval_weighted_avg_precision': 1.0, 'eval_weighted_avg_recall': 1.0, 'eval_weighted_avg_f1': 1.0, 'eval_weighted_avg_number': 800, 'eval_overall_accuracy': 1.0, 'eval_runtime': 1.7543, 'eval_samples_per_second': 456.031, 'eval_steps_per_second': 28.502, 'epoch': 24.0}\n",
      " 80%|█████████████████████████████▌       | 12072/15090 [48:21<10:17,  4.89it/s]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 29.90it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:1935] 2023-01-21 08:13:25,466 >> Saving model checkpoint to /root/data/outputs1/checkpoint-12072\n",
      "[INFO|configuration_utils.py:391] 2023-01-21 08:13:25,476 >> Configuration saved in /root/data/outputs1/checkpoint-12072/config.json\n",
      "[INFO|modeling_utils.py:1001] 2023-01-21 08:13:30,092 >> Model weights saved in /root/data/outputs1/checkpoint-12072/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2020] 2023-01-21 08:13:30,100 >> tokenizer config file saved in /root/data/outputs1/checkpoint-12072/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2026] 2023-01-21 08:13:30,107 >> Special tokens file saved in /root/data/outputs1/checkpoint-12072/special_tokens_map.json\n",
      "{'loss': 0.0016, 'learning_rate': 3.7037037037037037e-06, 'epoch': 25.0}        \n",
      " 83%|██████████████████████████████▊      | 12575/15090 [50:21<09:15,  4.53it/s][INFO|trainer.py:521] 2023-01-21 08:15:25,623 >> The following columns in the evaluation set  don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tags, tokens.\n",
      "[INFO|trainer.py:2181] 2023-01-21 08:15:25,626 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2183] 2023-01-21 08:15:25,626 >>   Num examples = 800\n",
      "[INFO|trainer.py:2186] 2023-01-21 08:15:25,626 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:00<00:01, 39.03it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:01, 36.68it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:01, 36.21it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:00<00:00, 35.33it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:00<00:00, 34.44it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:00<00:00, 33.77it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:00<00:00, 30.51it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:00<00:00, 29.71it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:01<00:00, 30.35it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:01<00:00, 30.11it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:01<00:00, 30.02it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:01<00:00, 29.57it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 9.547881927574053e-05, 'eval_CORP_precision': 1.0, 'eval_CORP_recall': 1.0, 'eval_CORP_f1': 1.0, 'eval_CORP_number': 127, 'eval_CW_precision': 1.0, 'eval_CW_recall': 1.0, 'eval_CW_f1': 1.0, 'eval_CW_number': 120, 'eval_GRP_precision': 1.0, 'eval_GRP_recall': 1.0, 'eval_GRP_f1': 1.0, 'eval_GRP_number': 118, 'eval_LOC_precision': 1.0, 'eval_LOC_recall': 1.0, 'eval_LOC_f1': 1.0, 'eval_LOC_number': 101, 'eval_PER_precision': 1.0, 'eval_PER_recall': 1.0, 'eval_PER_f1': 1.0, 'eval_PER_number': 144, 'eval_PROD_precision': 1.0, 'eval_PROD_recall': 1.0, 'eval_PROD_f1': 1.0, 'eval_PROD_number': 190, 'eval_micro_avg_precision': 1.0, 'eval_micro_avg_recall': 1.0, 'eval_micro_avg_f1': 1.0, 'eval_micro_avg_number': 800, 'eval_macro_avg_precision': 1.0, 'eval_macro_avg_recall': 1.0, 'eval_macro_avg_f1': 1.0, 'eval_macro_avg_number': 800, 'eval_weighted_avg_precision': 1.0, 'eval_weighted_avg_recall': 1.0, 'eval_weighted_avg_f1': 1.0, 'eval_weighted_avg_number': 800, 'eval_overall_accuracy': 1.0, 'eval_runtime': 1.7611, 'eval_samples_per_second': 454.263, 'eval_steps_per_second': 28.391, 'epoch': 25.0}\n",
      " 83%|██████████████████████████████▊      | 12575/15090 [50:23<09:15,  4.53it/s]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 30.21it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:1935] 2023-01-21 08:15:27,395 >> Saving model checkpoint to /root/data/outputs1/checkpoint-12575\n",
      "[INFO|configuration_utils.py:391] 2023-01-21 08:15:27,405 >> Configuration saved in /root/data/outputs1/checkpoint-12575/config.json\n",
      "[INFO|modeling_utils.py:1001] 2023-01-21 08:15:32,048 >> Model weights saved in /root/data/outputs1/checkpoint-12575/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2020] 2023-01-21 08:15:32,056 >> tokenizer config file saved in /root/data/outputs1/checkpoint-12575/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2026] 2023-01-21 08:15:32,063 >> Special tokens file saved in /root/data/outputs1/checkpoint-12575/special_tokens_map.json\n",
      "{'loss': 0.0012, 'learning_rate': 2.962962962962963e-06, 'epoch': 26.0}         \n",
      " 87%|████████████████████████████████     | 13078/15090 [52:23<07:13,  4.64it/s][INFO|trainer.py:521] 2023-01-21 08:17:27,659 >> The following columns in the evaluation set  don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tags, tokens.\n",
      "[INFO|trainer.py:2181] 2023-01-21 08:17:27,661 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2183] 2023-01-21 08:17:27,662 >>   Num examples = 800\n",
      "[INFO|trainer.py:2186] 2023-01-21 08:17:27,662 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:00<00:01, 40.35it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:01, 37.60it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:01, 36.92it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:00<00:00, 35.91it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:00<00:00, 34.51it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:00<00:00, 33.92it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:00<00:00, 30.97it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:00<00:00, 30.52it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:01<00:00, 31.14it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:01<00:00, 30.53it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:01<00:00, 30.29it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:01<00:00, 29.54it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 8.700793841853738e-05, 'eval_CORP_precision': 1.0, 'eval_CORP_recall': 1.0, 'eval_CORP_f1': 1.0, 'eval_CORP_number': 127, 'eval_CW_precision': 1.0, 'eval_CW_recall': 1.0, 'eval_CW_f1': 1.0, 'eval_CW_number': 120, 'eval_GRP_precision': 1.0, 'eval_GRP_recall': 1.0, 'eval_GRP_f1': 1.0, 'eval_GRP_number': 118, 'eval_LOC_precision': 1.0, 'eval_LOC_recall': 1.0, 'eval_LOC_f1': 1.0, 'eval_LOC_number': 101, 'eval_PER_precision': 1.0, 'eval_PER_recall': 1.0, 'eval_PER_f1': 1.0, 'eval_PER_number': 144, 'eval_PROD_precision': 1.0, 'eval_PROD_recall': 1.0, 'eval_PROD_f1': 1.0, 'eval_PROD_number': 190, 'eval_micro_avg_precision': 1.0, 'eval_micro_avg_recall': 1.0, 'eval_micro_avg_f1': 1.0, 'eval_micro_avg_number': 800, 'eval_macro_avg_precision': 1.0, 'eval_macro_avg_recall': 1.0, 'eval_macro_avg_f1': 1.0, 'eval_macro_avg_number': 800, 'eval_weighted_avg_precision': 1.0, 'eval_weighted_avg_recall': 1.0, 'eval_weighted_avg_f1': 1.0, 'eval_weighted_avg_number': 800, 'eval_overall_accuracy': 1.0, 'eval_runtime': 1.7489, 'eval_samples_per_second': 457.438, 'eval_steps_per_second': 28.59, 'epoch': 26.0}\n",
      " 87%|████████████████████████████████     | 13078/15090 [52:25<07:13,  4.64it/s]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 30.34it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:1935] 2023-01-21 08:17:29,419 >> Saving model checkpoint to /root/data/outputs1/checkpoint-13078\n",
      "[INFO|configuration_utils.py:391] 2023-01-21 08:17:29,429 >> Configuration saved in /root/data/outputs1/checkpoint-13078/config.json\n",
      "[INFO|modeling_utils.py:1001] 2023-01-21 08:17:34,007 >> Model weights saved in /root/data/outputs1/checkpoint-13078/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2020] 2023-01-21 08:17:34,018 >> tokenizer config file saved in /root/data/outputs1/checkpoint-13078/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2026] 2023-01-21 08:17:34,025 >> Special tokens file saved in /root/data/outputs1/checkpoint-13078/special_tokens_map.json\n",
      "{'loss': 0.0011, 'learning_rate': 2.222222222222222e-06, 'epoch': 27.0}         \n",
      " 90%|█████████████████████████████████▎   | 13581/15090 [54:25<05:14,  4.79it/s][INFO|trainer.py:521] 2023-01-21 08:19:29,914 >> The following columns in the evaluation set  don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tags, tokens.\n",
      "[INFO|trainer.py:2181] 2023-01-21 08:19:29,917 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2183] 2023-01-21 08:19:29,917 >>   Num examples = 800\n",
      "[INFO|trainer.py:2186] 2023-01-21 08:19:29,917 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:00<00:01, 40.96it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:01, 37.75it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:01, 36.67it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:00<00:00, 35.97it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:00<00:00, 34.66it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:00<00:00, 33.91it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:00<00:00, 31.00it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:00<00:00, 30.51it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:01<00:00, 31.21it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:01<00:00, 30.56it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:01<00:00, 30.33it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:01<00:00, 29.63it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 8.421803795499727e-05, 'eval_CORP_precision': 1.0, 'eval_CORP_recall': 1.0, 'eval_CORP_f1': 1.0, 'eval_CORP_number': 127, 'eval_CW_precision': 1.0, 'eval_CW_recall': 1.0, 'eval_CW_f1': 1.0, 'eval_CW_number': 120, 'eval_GRP_precision': 1.0, 'eval_GRP_recall': 1.0, 'eval_GRP_f1': 1.0, 'eval_GRP_number': 118, 'eval_LOC_precision': 1.0, 'eval_LOC_recall': 1.0, 'eval_LOC_f1': 1.0, 'eval_LOC_number': 101, 'eval_PER_precision': 1.0, 'eval_PER_recall': 1.0, 'eval_PER_f1': 1.0, 'eval_PER_number': 144, 'eval_PROD_precision': 1.0, 'eval_PROD_recall': 1.0, 'eval_PROD_f1': 1.0, 'eval_PROD_number': 190, 'eval_micro_avg_precision': 1.0, 'eval_micro_avg_recall': 1.0, 'eval_micro_avg_f1': 1.0, 'eval_micro_avg_number': 800, 'eval_macro_avg_precision': 1.0, 'eval_macro_avg_recall': 1.0, 'eval_macro_avg_f1': 1.0, 'eval_macro_avg_number': 800, 'eval_weighted_avg_precision': 1.0, 'eval_weighted_avg_recall': 1.0, 'eval_weighted_avg_f1': 1.0, 'eval_weighted_avg_number': 800, 'eval_overall_accuracy': 1.0, 'eval_runtime': 1.7487, 'eval_samples_per_second': 457.484, 'eval_steps_per_second': 28.593, 'epoch': 27.0}\n",
      " 90%|█████████████████████████████████▎   | 13581/15090 [54:27<05:14,  4.79it/s]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 30.24it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:1935] 2023-01-21 08:19:31,674 >> Saving model checkpoint to /root/data/outputs1/checkpoint-13581\n",
      "[INFO|configuration_utils.py:391] 2023-01-21 08:19:31,683 >> Configuration saved in /root/data/outputs1/checkpoint-13581/config.json\n",
      "[INFO|modeling_utils.py:1001] 2023-01-21 08:19:36,263 >> Model weights saved in /root/data/outputs1/checkpoint-13581/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2020] 2023-01-21 08:19:36,271 >> tokenizer config file saved in /root/data/outputs1/checkpoint-13581/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2026] 2023-01-21 08:19:36,278 >> Special tokens file saved in /root/data/outputs1/checkpoint-13581/special_tokens_map.json\n",
      "{'loss': 0.0009, 'learning_rate': 1.4814814814814815e-06, 'epoch': 28.0}        \n",
      " 93%|██████████████████████████████████▌  | 14084/15090 [56:27<03:27,  4.85it/s][INFO|trainer.py:521] 2023-01-21 08:21:31,362 >> The following columns in the evaluation set  don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tags, tokens.\n",
      "[INFO|trainer.py:2181] 2023-01-21 08:21:31,364 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2183] 2023-01-21 08:21:31,364 >>   Num examples = 800\n",
      "[INFO|trainer.py:2186] 2023-01-21 08:21:31,365 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:00<00:01, 41.45it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:01, 38.19it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:00, 37.45it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:00<00:00, 36.42it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:00<00:00, 35.20it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:00<00:00, 34.36it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:00<00:00, 30.96it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:00<00:00, 30.88it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:01<00:00, 31.03it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:01<00:00, 30.44it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:01<00:00, 30.85it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 7.832998380763456e-05, 'eval_CORP_precision': 1.0, 'eval_CORP_recall': 1.0, 'eval_CORP_f1': 1.0, 'eval_CORP_number': 127, 'eval_CW_precision': 1.0, 'eval_CW_recall': 1.0, 'eval_CW_f1': 1.0, 'eval_CW_number': 120, 'eval_GRP_precision': 1.0, 'eval_GRP_recall': 1.0, 'eval_GRP_f1': 1.0, 'eval_GRP_number': 118, 'eval_LOC_precision': 1.0, 'eval_LOC_recall': 1.0, 'eval_LOC_f1': 1.0, 'eval_LOC_number': 101, 'eval_PER_precision': 1.0, 'eval_PER_recall': 1.0, 'eval_PER_f1': 1.0, 'eval_PER_number': 144, 'eval_PROD_precision': 1.0, 'eval_PROD_recall': 1.0, 'eval_PROD_f1': 1.0, 'eval_PROD_number': 190, 'eval_micro_avg_precision': 1.0, 'eval_micro_avg_recall': 1.0, 'eval_micro_avg_f1': 1.0, 'eval_micro_avg_number': 800, 'eval_macro_avg_precision': 1.0, 'eval_macro_avg_recall': 1.0, 'eval_macro_avg_f1': 1.0, 'eval_macro_avg_number': 800, 'eval_weighted_avg_precision': 1.0, 'eval_weighted_avg_recall': 1.0, 'eval_weighted_avg_f1': 1.0, 'eval_weighted_avg_number': 800, 'eval_overall_accuracy': 1.0, 'eval_runtime': 1.7341, 'eval_samples_per_second': 461.329, 'eval_steps_per_second': 28.833, 'epoch': 28.0}\n",
      " 93%|██████████████████████████████████▌  | 14084/15090 [56:29<03:27,  4.85it/s]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 30.39it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:1935] 2023-01-21 08:21:33,108 >> Saving model checkpoint to /root/data/outputs1/checkpoint-14084\n",
      "[INFO|configuration_utils.py:391] 2023-01-21 08:21:33,117 >> Configuration saved in /root/data/outputs1/checkpoint-14084/config.json\n",
      "[INFO|modeling_utils.py:1001] 2023-01-21 08:21:37,717 >> Model weights saved in /root/data/outputs1/checkpoint-14084/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2020] 2023-01-21 08:21:37,727 >> tokenizer config file saved in /root/data/outputs1/checkpoint-14084/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2026] 2023-01-21 08:21:37,734 >> Special tokens file saved in /root/data/outputs1/checkpoint-14084/special_tokens_map.json\n",
      "{'loss': 0.0007, 'learning_rate': 7.407407407407407e-07, 'epoch': 29.0}         \n",
      " 97%|███████████████████████████████████▊ | 14587/15090 [58:28<01:43,  4.88it/s][INFO|trainer.py:521] 2023-01-21 08:23:32,879 >> The following columns in the evaluation set  don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tags, tokens.\n",
      "[INFO|trainer.py:2181] 2023-01-21 08:23:32,881 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2183] 2023-01-21 08:23:32,882 >>   Num examples = 800\n",
      "[INFO|trainer.py:2186] 2023-01-21 08:23:32,882 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:00<00:01, 40.56it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:01, 37.51it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:01, 36.73it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:00<00:00, 36.03it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:00<00:00, 34.74it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:00<00:00, 34.20it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:00<00:00, 31.10it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:00<00:00, 30.44it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:01<00:00, 30.93it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:01<00:00, 30.43it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:01<00:00, 30.42it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:01<00:00, 29.91it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 7.66303128330037e-05, 'eval_CORP_precision': 1.0, 'eval_CORP_recall': 1.0, 'eval_CORP_f1': 1.0, 'eval_CORP_number': 127, 'eval_CW_precision': 1.0, 'eval_CW_recall': 1.0, 'eval_CW_f1': 1.0, 'eval_CW_number': 120, 'eval_GRP_precision': 1.0, 'eval_GRP_recall': 1.0, 'eval_GRP_f1': 1.0, 'eval_GRP_number': 118, 'eval_LOC_precision': 1.0, 'eval_LOC_recall': 1.0, 'eval_LOC_f1': 1.0, 'eval_LOC_number': 101, 'eval_PER_precision': 1.0, 'eval_PER_recall': 1.0, 'eval_PER_f1': 1.0, 'eval_PER_number': 144, 'eval_PROD_precision': 1.0, 'eval_PROD_recall': 1.0, 'eval_PROD_f1': 1.0, 'eval_PROD_number': 190, 'eval_micro_avg_precision': 1.0, 'eval_micro_avg_recall': 1.0, 'eval_micro_avg_f1': 1.0, 'eval_micro_avg_number': 800, 'eval_macro_avg_precision': 1.0, 'eval_macro_avg_recall': 1.0, 'eval_macro_avg_f1': 1.0, 'eval_macro_avg_number': 800, 'eval_weighted_avg_precision': 1.0, 'eval_weighted_avg_recall': 1.0, 'eval_weighted_avg_f1': 1.0, 'eval_weighted_avg_number': 800, 'eval_overall_accuracy': 1.0, 'eval_runtime': 1.7423, 'eval_samples_per_second': 459.173, 'eval_steps_per_second': 28.698, 'epoch': 29.0}\n",
      " 97%|███████████████████████████████████▊ | 14587/15090 [58:30<01:43,  4.88it/s]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 30.44it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:1935] 2023-01-21 08:23:34,632 >> Saving model checkpoint to /root/data/outputs1/checkpoint-14587\n",
      "[INFO|configuration_utils.py:391] 2023-01-21 08:23:34,642 >> Configuration saved in /root/data/outputs1/checkpoint-14587/config.json\n",
      "[INFO|modeling_utils.py:1001] 2023-01-21 08:23:39,248 >> Model weights saved in /root/data/outputs1/checkpoint-14587/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2020] 2023-01-21 08:23:39,256 >> tokenizer config file saved in /root/data/outputs1/checkpoint-14587/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2026] 2023-01-21 08:23:39,263 >> Special tokens file saved in /root/data/outputs1/checkpoint-14587/special_tokens_map.json\n",
      "{'loss': 0.0008, 'learning_rate': 0.0, 'epoch': 30.0}                           \n",
      "100%|███████████████████████████████████| 15090/15090 [1:00:29<00:00,  4.76it/s][INFO|trainer.py:521] 2023-01-21 08:25:34,005 >> The following columns in the evaluation set  don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tags, tokens.\n",
      "[INFO|trainer.py:2181] 2023-01-21 08:25:34,007 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2183] 2023-01-21 08:25:34,007 >>   Num examples = 800\n",
      "[INFO|trainer.py:2186] 2023-01-21 08:25:34,008 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:00<00:01, 40.06it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:01, 37.34it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:01, 36.40it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:00<00:00, 35.14it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:00<00:00, 34.36it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:00<00:00, 33.68it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:00<00:00, 30.58it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:00<00:00, 30.07it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:01<00:00, 30.58it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:01<00:00, 30.32it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:01<00:00, 30.12it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:01<00:00, 29.37it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 7.635264046257362e-05, 'eval_CORP_precision': 1.0, 'eval_CORP_recall': 1.0, 'eval_CORP_f1': 1.0, 'eval_CORP_number': 127, 'eval_CW_precision': 1.0, 'eval_CW_recall': 1.0, 'eval_CW_f1': 1.0, 'eval_CW_number': 120, 'eval_GRP_precision': 1.0, 'eval_GRP_recall': 1.0, 'eval_GRP_f1': 1.0, 'eval_GRP_number': 118, 'eval_LOC_precision': 1.0, 'eval_LOC_recall': 1.0, 'eval_LOC_f1': 1.0, 'eval_LOC_number': 101, 'eval_PER_precision': 1.0, 'eval_PER_recall': 1.0, 'eval_PER_f1': 1.0, 'eval_PER_number': 144, 'eval_PROD_precision': 1.0, 'eval_PROD_recall': 1.0, 'eval_PROD_f1': 1.0, 'eval_PROD_number': 190, 'eval_micro_avg_precision': 1.0, 'eval_micro_avg_recall': 1.0, 'eval_micro_avg_f1': 1.0, 'eval_micro_avg_number': 800, 'eval_macro_avg_precision': 1.0, 'eval_macro_avg_recall': 1.0, 'eval_macro_avg_f1': 1.0, 'eval_macro_avg_number': 800, 'eval_weighted_avg_precision': 1.0, 'eval_weighted_avg_recall': 1.0, 'eval_weighted_avg_f1': 1.0, 'eval_weighted_avg_number': 800, 'eval_overall_accuracy': 1.0, 'eval_runtime': 1.7662, 'eval_samples_per_second': 452.953, 'eval_steps_per_second': 28.31, 'epoch': 30.0}\n",
      "100%|███████████████████████████████████| 15090/15090 [1:00:31<00:00,  4.76it/s]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 29.88it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:1935] 2023-01-21 08:25:35,781 >> Saving model checkpoint to /root/data/outputs1/checkpoint-15090\n",
      "[INFO|configuration_utils.py:391] 2023-01-21 08:25:35,791 >> Configuration saved in /root/data/outputs1/checkpoint-15090/config.json\n",
      "[INFO|modeling_utils.py:1001] 2023-01-21 08:25:40,423 >> Model weights saved in /root/data/outputs1/checkpoint-15090/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2020] 2023-01-21 08:25:40,432 >> tokenizer config file saved in /root/data/outputs1/checkpoint-15090/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2026] 2023-01-21 08:25:40,438 >> Special tokens file saved in /root/data/outputs1/checkpoint-15090/special_tokens_map.json\n",
      "[INFO|trainer.py:1366] 2023-01-21 08:25:49,971 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 3657.3089, 'train_samples_per_second': 132.064, 'train_steps_per_second': 4.126, 'train_loss': 0.06758471316972105, 'epoch': 30.0}\n",
      "100%|███████████████████████████████████| 15090/15090 [1:00:45<00:00,  4.14it/s]\n",
      "[INFO|trainer.py:1935] 2023-01-21 08:25:49,975 >> Saving model checkpoint to /root/data/outputs1\n",
      "[INFO|configuration_utils.py:391] 2023-01-21 08:25:49,983 >> Configuration saved in /root/data/outputs1/config.json\n",
      "[INFO|modeling_utils.py:1001] 2023-01-21 08:25:54,610 >> Model weights saved in /root/data/outputs1/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2020] 2023-01-21 08:25:54,618 >> tokenizer config file saved in /root/data/outputs1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2026] 2023-01-21 08:25:54,626 >> Special tokens file saved in /root/data/outputs1/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =       30.0\n",
      "  train_loss               =     0.0676\n",
      "  train_runtime            = 1:00:57.30\n",
      "  train_samples            =      16100\n",
      "  train_samples_per_second =    132.064\n",
      "  train_steps_per_second   =      4.126\n",
      "01/21/2023 08:25:54 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:521] 2023-01-21 08:25:54,779 >> The following columns in the evaluation set  don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tags, tokens.\n",
      "[INFO|trainer.py:2181] 2023-01-21 08:25:54,781 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2183] 2023-01-21 08:25:54,781 >>   Num examples = 800\n",
      "[INFO|trainer.py:2186] 2023-01-21 08:25:54,781 >>   Batch size = 16\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 28.76it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                       =       30.0\n",
      "  eval_CORP_f1                =        1.0\n",
      "  eval_CORP_number            =        127\n",
      "  eval_CORP_precision         =        1.0\n",
      "  eval_CORP_recall            =        1.0\n",
      "  eval_CW_f1                  =        1.0\n",
      "  eval_CW_number              =        120\n",
      "  eval_CW_precision           =        1.0\n",
      "  eval_CW_recall              =        1.0\n",
      "  eval_GRP_f1                 =        1.0\n",
      "  eval_GRP_number             =        118\n",
      "  eval_GRP_precision          =        1.0\n",
      "  eval_GRP_recall             =        1.0\n",
      "  eval_LOC_f1                 =        1.0\n",
      "  eval_LOC_number             =        101\n",
      "  eval_LOC_precision          =        1.0\n",
      "  eval_LOC_recall             =        1.0\n",
      "  eval_PER_f1                 =        1.0\n",
      "  eval_PER_number             =        144\n",
      "  eval_PER_precision          =        1.0\n",
      "  eval_PER_recall             =        1.0\n",
      "  eval_PROD_f1                =        1.0\n",
      "  eval_PROD_number            =        190\n",
      "  eval_PROD_precision         =        1.0\n",
      "  eval_PROD_recall            =        1.0\n",
      "  eval_loss                   =     0.0001\n",
      "  eval_macro_avg_f1           =        1.0\n",
      "  eval_macro_avg_number       =        800\n",
      "  eval_macro_avg_precision    =        1.0\n",
      "  eval_macro_avg_recall       =        1.0\n",
      "  eval_micro_avg_f1           =        1.0\n",
      "  eval_micro_avg_number       =        800\n",
      "  eval_micro_avg_precision    =        1.0\n",
      "  eval_micro_avg_recall       =        1.0\n",
      "  eval_overall_accuracy       =        1.0\n",
      "  eval_runtime                = 0:00:01.78\n",
      "  eval_samples                =        800\n",
      "  eval_samples_per_second     =    447.109\n",
      "  eval_steps_per_second       =     27.944\n",
      "  eval_weighted_avg_f1        =        1.0\n",
      "  eval_weighted_avg_number    =        800\n",
      "  eval_weighted_avg_precision =        1.0\n",
      "  eval_weighted_avg_recall    =        1.0\n",
      "01/21/2023 08:25:56 - INFO - __main__ - *** Predict ***\n",
      "[INFO|trainer.py:521] 2023-01-21 08:25:56,595 >> The following columns in the test set  don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tags, tokens.\n",
      "[INFO|trainer.py:2181] 2023-01-21 08:25:56,598 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2183] 2023-01-21 08:25:56,598 >>   Num examples = 800\n",
      "[INFO|trainer.py:2186] 2023-01-21 08:25:56,599 >>   Batch size = 16\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:01<00:00, 30.77it/s]***** predict metrics *****\n",
      "  predict_CORP_f1                =        1.0\n",
      "  predict_CORP_number            =        127\n",
      "  predict_CORP_precision         =        1.0\n",
      "  predict_CORP_recall            =        1.0\n",
      "  predict_CW_f1                  =        1.0\n",
      "  predict_CW_number              =        120\n",
      "  predict_CW_precision           =        1.0\n",
      "  predict_CW_recall              =        1.0\n",
      "  predict_GRP_f1                 =        1.0\n",
      "  predict_GRP_number             =        118\n",
      "  predict_GRP_precision          =        1.0\n",
      "  predict_GRP_recall             =        1.0\n",
      "  predict_LOC_f1                 =        1.0\n",
      "  predict_LOC_number             =        101\n",
      "  predict_LOC_precision          =        1.0\n",
      "  predict_LOC_recall             =        1.0\n",
      "  predict_PER_f1                 =        1.0\n",
      "  predict_PER_number             =        144\n",
      "  predict_PER_precision          =        1.0\n",
      "  predict_PER_recall             =        1.0\n",
      "  predict_PROD_f1                =        1.0\n",
      "  predict_PROD_number            =        190\n",
      "  predict_PROD_precision         =        1.0\n",
      "  predict_PROD_recall            =        1.0\n",
      "  predict_loss                   =     0.0001\n",
      "  predict_macro_avg_f1           =        1.0\n",
      "  predict_macro_avg_number       =        800\n",
      "  predict_macro_avg_precision    =        1.0\n",
      "  predict_macro_avg_recall       =        1.0\n",
      "  predict_micro_avg_f1           =        1.0\n",
      "  predict_micro_avg_number       =        800\n",
      "  predict_micro_avg_precision    =        1.0\n",
      "  predict_micro_avg_recall       =        1.0\n",
      "  predict_overall_accuracy       =        1.0\n",
      "  predict_runtime                = 0:00:01.71\n",
      "  predict_samples_per_second     =    466.215\n",
      "  predict_steps_per_second       =     29.138\n",
      "  predict_weighted_avg_f1        =        1.0\n",
      "  predict_weighted_avg_number    =        800\n",
      "  predict_weighted_avg_precision =        1.0\n",
      "  predict_weighted_avg_recall    =        1.0\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 28.82it/s]\n"
     ]
    }
   ],
   "source": [
    "!python token_classification.py \\\n",
    "    --model_name_or_path \"csebuetnlp/banglabert\" \\\n",
    "    --dataset_dir \"/root/data/\" \\\n",
    "    --output_dir \"/root/data/outputs1\" \\\n",
    "    --learning_rate=2e-5 \\\n",
    "    --warmup_ratio 0.1 \\\n",
    "    --gradient_accumulation_steps 2 \\\n",
    "    --weight_decay 0.1 \\\n",
    "    --lr_scheduler_type \"linear\"  \\\n",
    "    --per_device_train_batch_size=16 \\\n",
    "    --per_device_eval_batch_size=16 \\\n",
    "    --max_seq_length 512 \\\n",
    "    --logging_strategy \"epoch\" \\\n",
    "    --save_strategy \"epoch\" \\\n",
    "    --evaluation_strategy \"epoch\" \\\n",
    "    --do_eval --do_train --do_predict\\\n",
    "    --num_train_epochs=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/banglabert\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/banglabert/token_classification\n"
     ]
    }
   ],
   "source": [
    "%cd /root/banglabert/token_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01/21/2023 09:39:03 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "01/21/2023 09:39:03 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=False,\n",
      "do_predict=True,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/root/data/outputs11/runs/Jan21_09-39-03_datascience-1-0-ml-g4dn-xlarge-94fad2f4401e538ca1255dfa1e84,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "output_dir=/root/data/outputs11/,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=16,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=outputs11,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=['wandb'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/root/data/outputs11/,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "01/21/2023 09:39:03 - WARNING - datasets.builder - Using custom data configuration default-5c0d84e5c3bbe857\n",
      "01/21/2023 09:39:03 - INFO - datasets.builder - Generating dataset json (/root/.cache/huggingface/datasets/json/default-5c0d84e5c3bbe857/0.0.0)\n",
      "Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/json/default-5c0d84e5c3bbe857/0.0.0...\n",
      "100%|██████████████████████████████████████████| 3/3 [00:00<00:00, 12684.39it/s]\n",
      "01/21/2023 09:39:03 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n",
      "01/21/2023 09:39:03 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n",
      "100%|████████████████████████████████████████████| 3/3 [00:00<00:00, 137.70it/s]\n",
      "01/21/2023 09:39:04 - INFO - datasets.builder - Generating split train\n",
      "01/21/2023 09:39:04 - INFO - datasets.builder - Generating split validation\n",
      "01/21/2023 09:39:04 - INFO - datasets.builder - Generating split test\n",
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-5c0d84e5c3bbe857/0.0.0. Subsequent calls will reuse this data.\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:00<00:00, 93.16it/s]\n",
      "01/21/2023 09:39:04 - WARNING - datasets.fingerprint - Parameter 'column_names'=set() of the transform datasets.arrow_dataset.Dataset.remove_columns couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "01/21/2023 09:39:04 - INFO - datasets.fingerprint - Parameter 'column_names'=set() of the transform datasets.arrow_dataset.Dataset.remove_columns couldn't be hashed properly, a random hash was used instead.\n",
      "01/21/2023 09:39:04 - INFO - datasets.fingerprint - Parameter 'column_names'=set() of the transform datasets.arrow_dataset.Dataset.remove_columns couldn't be hashed properly, a random hash was used instead.\n",
      "[INFO|configuration_utils.py:559] 2023-01-21 09:39:04,261 >> loading configuration file /root/data/outputs1/config.json\n",
      "[INFO|configuration_utils.py:598] 2023-01-21 09:39:04,262 >> Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"csebuetnlp/banglabert\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"B-CORP\",\n",
      "    \"1\": \"B-CW\",\n",
      "    \"2\": \"B-GRP\",\n",
      "    \"3\": \"B-LOC\",\n",
      "    \"4\": \"B-PER\",\n",
      "    \"5\": \"B-PROD\",\n",
      "    \"6\": \"I-CORP\",\n",
      "    \"7\": \"I-CW\",\n",
      "    \"8\": \"I-GRP\",\n",
      "    \"9\": \"I-LOC\",\n",
      "    \"10\": \"I-PER\",\n",
      "    \"11\": \"I-PROD\",\n",
      "    \"12\": \"O\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"B-CORP\": 0,\n",
      "    \"B-CW\": 1,\n",
      "    \"B-GRP\": 2,\n",
      "    \"B-LOC\": 3,\n",
      "    \"B-PER\": 4,\n",
      "    \"B-PROD\": 5,\n",
      "    \"I-CORP\": 6,\n",
      "    \"I-CW\": 7,\n",
      "    \"I-GRP\": 8,\n",
      "    \"I-LOC\": 9,\n",
      "    \"I-PER\": 10,\n",
      "    \"I-PROD\": 11,\n",
      "    \"O\": 12\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"finetuned\": true\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.11.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1669] 2023-01-21 09:39:04,291 >> Didn't find file /root/data/outputs1/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1737] 2023-01-21 09:39:04,291 >> loading file /root/data/outputs1/vocab.txt\n",
      "[INFO|tokenization_utils_base.py:1737] 2023-01-21 09:39:04,291 >> loading file /root/data/outputs1/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1737] 2023-01-21 09:39:04,291 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1737] 2023-01-21 09:39:04,291 >> loading file /root/data/outputs1/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1737] 2023-01-21 09:39:04,291 >> loading file /root/data/outputs1/tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1277] 2023-01-21 09:39:04,333 >> loading weights file /root/data/outputs1/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1524] 2023-01-21 09:39:05,525 >> All model checkpoint weights were used when initializing ElectraForTokenClassification.\n",
      "\n",
      "[INFO|modeling_utils.py:1533] 2023-01-21 09:39:05,525 >> All the weights of ElectraForTokenClassification were initialized from the model checkpoint at /root/data/outputs1/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ElectraForTokenClassification for predictions without further training.\n",
      "01/21/2023 09:39:05 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.normalize_example at 0x7fdcafaa1440> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n",
      "Running normalization on dataset:   0%|               | 0/16100 [00:00<?, ?ex/s]01/21/2023 09:39:05 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-5c0d84e5c3bbe857/0.0.0/cache-23b8c1e9392456de.arrow\n",
      "Running normalization on dataset: 100%|█| 16100/16100 [00:08<00:00, 1870.14ex/s]\n",
      "01/21/2023 09:39:14 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.normalize_example at 0x7fdcafaa1440> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n",
      "Running normalization on dataset:   0%|                 | 0/800 [00:00<?, ?ex/s]01/21/2023 09:39:14 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-5c0d84e5c3bbe857/0.0.0/cache-1a3d1fa7bc8960a9.arrow\n",
      "Running normalization on dataset: 100%|█████| 800/800 [00:00<00:00, 1530.96ex/s]\n",
      "01/21/2023 09:39:14 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.normalize_example at 0x7fdcafaa1440> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n",
      "Running normalization on dataset:   0%|               | 0/13215 [00:00<?, ?ex/s]01/21/2023 09:39:14 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-5c0d84e5c3bbe857/0.0.0/cache-bd9c66b3ad3c2d6d.arrow\n",
      "Running normalization on dataset: 100%|█| 13215/13215 [00:06<00:00, 2020.59ex/s]\n",
      "01/21/2023 09:39:21 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.tokenize_and_align_labels at 0x7fdcafa58cb0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n",
      "Running tokenizer on dataset:   0%|                      | 0/17 [00:00<?, ?ba/s]01/21/2023 09:39:21 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-5c0d84e5c3bbe857/0.0.0/cache-8b9d2434e465e150.arrow\n",
      "Running tokenizer on dataset: 100%|█████████████| 17/17 [00:01<00:00, 10.74ba/s]\n",
      "01/21/2023 09:39:23 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.tokenize_and_align_labels at 0x7fdcafa58cb0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n",
      "Running tokenizer on dataset:   0%|                       | 0/1 [00:00<?, ?ba/s]01/21/2023 09:39:23 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-5c0d84e5c3bbe857/0.0.0/cache-972a846916419f82.arrow\n",
      "Running tokenizer on dataset: 100%|███████████████| 1/1 [00:00<00:00, 12.68ba/s]\n",
      "01/21/2023 09:39:23 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.tokenize_and_align_labels at 0x7fdcafa58cb0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n",
      "Running tokenizer on dataset:   0%|                      | 0/14 [00:00<?, ?ba/s]01/21/2023 09:39:23 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-5c0d84e5c3bbe857/0.0.0/cache-0822e8f36c031199.arrow\n",
      "Running tokenizer on dataset: 100%|█████████████| 14/14 [00:01<00:00, 11.93ba/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "01/21/2023 09:39:27 - INFO - __main__ - *** Predict ***\n",
      "[INFO|trainer.py:521] 2023-01-21 09:39:27,380 >> The following columns in the test set  don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tags, tokens.\n",
      "[INFO|trainer.py:2181] 2023-01-21 09:39:27,384 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2183] 2023-01-21 09:39:27,384 >>   Num examples = 13215\n",
      "[INFO|trainer.py:2186] 2023-01-21 09:39:27,384 >>   Batch size = 16\n",
      "100%|████████████████████████████████████████▊| 823/826 [00:22<00:00, 35.94it/s]/opt/conda/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "***** predict metrics *****\n",
      "  predict_CORP_f1                =        0.0\n",
      "  predict_CORP_number            =          0\n",
      "  predict_CORP_precision         =        0.0\n",
      "  predict_CORP_recall            =        0.0\n",
      "  predict_CW_f1                  =        0.0\n",
      "  predict_CW_number              =          0\n",
      "  predict_CW_precision           =        0.0\n",
      "  predict_CW_recall              =        0.0\n",
      "  predict_GRP_f1                 =        0.0\n",
      "  predict_GRP_number             =          0\n",
      "  predict_GRP_precision          =        0.0\n",
      "  predict_GRP_recall             =        0.0\n",
      "  predict_LOC_f1                 =        0.0\n",
      "  predict_LOC_number             =          0\n",
      "  predict_LOC_precision          =        0.0\n",
      "  predict_LOC_recall             =        0.0\n",
      "  predict_PER_f1                 =        0.0\n",
      "  predict_PER_number             =          0\n",
      "  predict_PER_precision          =        0.0\n",
      "  predict_PER_recall             =        0.0\n",
      "  predict_PROD_f1                =        0.0\n",
      "  predict_PROD_number            =          0\n",
      "  predict_PROD_precision         =        0.0\n",
      "  predict_PROD_recall            =        0.0\n",
      "  predict_loss                   =     1.7479\n",
      "  predict_macro_avg_f1           =        0.0\n",
      "  predict_macro_avg_number       =          0\n",
      "  predict_macro_avg_precision    =        0.0\n",
      "  predict_macro_avg_recall       =        0.0\n",
      "  predict_micro_avg_f1           =        0.0\n",
      "  predict_micro_avg_number       =          0\n",
      "  predict_micro_avg_precision    =        0.0\n",
      "  predict_micro_avg_recall       =        0.0\n",
      "  predict_overall_accuracy       =     0.8049\n",
      "  predict_runtime                = 0:00:25.94\n",
      "  predict_samples_per_second     =    509.315\n",
      "  predict_steps_per_second       =     31.835\n",
      "  predict_weighted_avg_f1        =        0.0\n",
      "  predict_weighted_avg_number    =          0\n",
      "  predict_weighted_avg_precision =        0.0\n",
      "  predict_weighted_avg_recall    =        0.0\n",
      "100%|█████████████████████████████████████████| 826/826 [00:25<00:00, 32.34it/s]\n"
     ]
    }
   ],
   "source": [
    "!python token_classification.py \\\n",
    "    --model_name_or_path \"/root/data/outputs1/\" \\\n",
    "    --dataset_dir \"/root/data/\" \\\n",
    "    --output_dir \"/root/data/outputs11/\" \\\n",
    "    --per_device_eval_batch_size=16 \\\n",
    "    --overwrite_output_dir \\\n",
    "    --do_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "data = []\n",
    "with io.open(\"/root/data/outputs11/predictions.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        fields = line.strip().split(\" \")\n",
    "        for field in fields:\n",
    "            data.append(field)\n",
    "        data.append(\"\")\n",
    "\n",
    "# write the list line by line to a file\n",
    "with io.open(\"/root/pred_labels.txt\", 'w', encoding=\"utf-8\") as f:\n",
    "    for item in data:\n",
    "        f.write(item+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
